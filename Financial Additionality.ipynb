{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Addionality\n",
    "This notebook goes through a series of API's and creates several .csv files.\n",
    "\n",
    "Currently implemented:\n",
    "\n",
    "* [x] GFDD\n",
    "* [x] GFDD_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import getpass\n",
    "import gzip\n",
    "import io\n",
    "from urllib.parse import quote\n",
    "from io import StringIO, BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"\"\n",
    "# Ask for client secret as password\n",
    "client_secret = getpass.getpass(\"Enter client secret: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>incomegroup</th>\n",
       "      <th>ifs</th>\n",
       "      <th>imf_region</th>\n",
       "      <th>imf_income</th>\n",
       "      <th>code_2</th>\n",
       "      <th>country_unctad</th>\n",
       "      <th>country_vcpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>Low income</td>\n",
       "      <td>512.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALB</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Europe &amp; Central Asia</td>\n",
       "      <td>Upper middle income</td>\n",
       "      <td>914.0</td>\n",
       "      <td>Europe</td>\n",
       "      <td>EM</td>\n",
       "      <td>AL</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DZA</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Middle East &amp; North Africa</td>\n",
       "      <td>Lower middle income</td>\n",
       "      <td>612.0</td>\n",
       "      <td>Middle East and Central Asia</td>\n",
       "      <td>EM</td>\n",
       "      <td>DZ</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGO</td>\n",
       "      <td>Angola</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>Lower middle income</td>\n",
       "      <td>614.0</td>\n",
       "      <td>Africa</td>\n",
       "      <td>EM</td>\n",
       "      <td>AO</td>\n",
       "      <td>Angola</td>\n",
       "      <td>Angola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Latin America &amp; Caribbean</td>\n",
       "      <td>Upper middle income</td>\n",
       "      <td>213.0</td>\n",
       "      <td>Western Hemisphere</td>\n",
       "      <td>EM</td>\n",
       "      <td>AR</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code      country                      region          incomegroup    ifs  \\\n",
       "0  AFG  Afghanistan                  South Asia           Low income  512.0   \n",
       "1  ALB      Albania       Europe & Central Asia  Upper middle income  914.0   \n",
       "2  DZA      Algeria  Middle East & North Africa  Lower middle income  612.0   \n",
       "3  AGO       Angola          Sub-Saharan Africa  Lower middle income  614.0   \n",
       "4  ARG    Argentina   Latin America & Caribbean  Upper middle income  213.0   \n",
       "\n",
       "                     imf_region imf_income code_2 country_unctad country_vcpe  \n",
       "0                                              AF    Afghanistan               \n",
       "1                        Europe         EM     AL        Albania      Albania  \n",
       "2  Middle East and Central Asia         EM     DZ        Algeria      Algeria  \n",
       "3                        Africa         EM     AO         Angola       Angola  \n",
       "4            Western Hemisphere         EM     AR      Argentina    Argentina  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hard-coded DataFrame with country data\n",
    "data = [\n",
    "    ['AFG', 'Afghanistan', 'South Asia', 'Low income', 512, '', '', 'AF', 'Afghanistan', ''],\n",
    "    ['ALB', 'Albania', 'Europe & Central Asia', 'Upper middle income', 914, 'Europe', 'EM', 'AL', 'Albania', 'Albania'],\n",
    "    ['DZA', 'Algeria', 'Middle East & North Africa', 'Lower middle income', 612, 'Middle East and Central Asia', 'EM', 'DZ', 'Algeria', 'Algeria'],\n",
    "    ['AGO', 'Angola', 'Sub-Saharan Africa', 'Lower middle income', 614, 'Africa', 'EM', 'AO', 'Angola', 'Angola'],\n",
    "    ['ARG', 'Argentina', 'Latin America & Caribbean', 'Upper middle income', 213, 'Western Hemisphere', 'EM', 'AR', 'Argentina', 'Argentina'],\n",
    "    ['ARM', 'Armenia', 'Europe & Central Asia', 'Upper middle income', 911, 'Middle East and Central Asia', 'EM', 'AM', 'Armenia', 'Armenia'],\n",
    "    ['AZE', 'Azerbaijan', 'Europe & Central Asia', 'Upper middle income', 912, 'Middle East and Central Asia', 'EM', 'AZ', 'Azerbaijan', 'Azerbaijan'],\n",
    "    ['BGD', 'Bangladesh', 'South Asia', 'Lower middle income', 513, 'Asia and Pacific', 'LIC', 'BD', 'Bangladesh', 'Bangladesh'],\n",
    "    ['BLR', 'Belarus', 'Europe & Central Asia', 'Upper middle income', 913, 'Europe', 'EM', 'BY', 'Belarus', 'Belarus'],\n",
    "    ['BLZ', 'Belize', 'Latin America & Caribbean', 'Upper middle income', 339, 'Western Hemisphere', 'EM', 'BZ', 'Belize', ''],\n",
    "    ['BEN', 'Benin', 'Sub-Saharan Africa', 'Lower middle income', 638, 'Africa', 'LIC', 'BJ', 'Benin', 'Benin'],\n",
    "    ['BTN', 'Bhutan', 'South Asia', 'Lower middle income', 514, 'Asia and Pacific', 'LIC', 'BT', 'Bhutan', ''],\n",
    "    ['BOL', 'Bolivia', 'Latin America & Caribbean', 'Lower middle income', 218, 'Western Hemisphere', 'EM', 'BO', 'Bolivia (Plurinational State of)', 'Bolivia'],\n",
    "    ['BIH', 'Bosnia and Herzegovina', 'Europe & Central Asia', 'Upper middle income', 963, 'Europe', 'EM', 'BA', 'Bosnia and Herzegovina', 'Bosnia-Herzegovina'],\n",
    "    ['BWA', 'Botswana', 'Sub-Saharan Africa', 'Upper middle income', 616, 'Africa', 'EM', 'BW', 'Botswana', 'Botswana'],\n",
    "    ['BRA', 'Brazil', 'Latin America & Caribbean', 'Upper middle income', 223, 'Western Hemisphere', 'EM', 'BR', 'Brazil', 'Brazil'],\n",
    "    ['BGR', 'Bulgaria', 'Europe & Central Asia', 'Upper middle income', 918, 'Europe', 'EM', 'BG', 'Bulgaria', 'Bulgaria'],\n",
    "    ['BFA', 'Burkina Faso', 'Sub-Saharan Africa', 'Low income', 748, 'Africa', 'LIC', 'BF', 'Burkina Faso', 'Burkina Faso'],\n",
    "    ['BDI', 'Burundi', 'Sub-Saharan Africa', 'Low income', 618, 'Africa', 'LIC', 'BI', 'Burundi', 'Burundi'],\n",
    "    ['CPV', 'Cabo Verde', 'Sub-Saharan Africa', 'Lower middle income', 624, 'Africa', 'LIC', 'CV', 'Cabo Verde', ''],\n",
    "    ['KHM', 'Cambodia', 'East Asia & Pacific', 'Lower middle income', 522, 'Asia and Pacific', 'LIC', 'KH', 'Cambodia', 'Cambodia'],\n",
    "    ['CMR', 'Cameroon', 'Sub-Saharan Africa', 'Lower middle income', 622, 'Africa', 'LIC', 'CM', 'Cameroon', 'Cameroon'],\n",
    "    ['CAF', 'Central African Republic', 'Sub-Saharan Africa', 'Low income', 626, 'Africa', 'LIC', 'CF', 'Central African Republic', ''],\n",
    "    ['TCD', 'Chad', 'Sub-Saharan Africa', 'Low income', 628, 'Africa', 'LIC', 'TD', 'Chad', 'Chad'],\n",
    "    ['CHN', 'China', 'East Asia & Pacific', 'Upper middle income', 924, 'Asia and Pacific', 'EM', 'CN', 'China', 'China'],\n",
    "    ['COL', 'Colombia', 'Latin America & Caribbean', 'Upper middle income', 233, 'Western Hemisphere', 'EM', 'CO', 'Colombia', 'Colombia'],\n",
    "    ['COM', 'Comoros', 'Sub-Saharan Africa', 'Lower middle income', 632, 'Africa', 'LIC', 'KM', 'Comoros', ''],\n",
    "    ['COD', 'Congo, Dem. Rep.', 'Sub-Saharan Africa', 'Low income', 636, 'Africa', 'LIC', 'CD', 'Congo, Dem. Rep. of the', ''],\n",
    "    ['COG', 'Congo, Rep.', 'Sub-Saharan Africa', 'Lower middle income', 634, 'Africa', 'LIC', 'CG', 'Congo', ''],\n",
    "    ['CRI', 'Costa Rica', 'Latin America & Caribbean', 'Upper middle income', 238, 'Western Hemisphere', 'EM', 'CR', 'Costa Rica', ''],\n",
    "    ['CIV', 'Côte d\\'Ivoire', 'Sub-Saharan Africa', 'Lower middle income', 662, 'Africa', 'LIC', 'CI', 'Cote d\\'Ivoire', 'Ivory Coast'],\n",
    "    ['CUB', 'Cuba', 'Latin America & Caribbean', 'Upper middle income', 928, '', '', 'CU', 'Cuba', ''],\n",
    "    ['DJI', 'Djibouti', 'Middle East & North Africa', 'Lower middle income', 611, 'Middle East and Central Asia', 'LIC', 'DJ', 'Djibouti', ''],\n",
    "    ['DMA', 'Dominica', 'Latin America & Caribbean', 'Upper middle income', 321, 'Western Hemisphere', 'LIC', 'DM', 'Dominica', ''],\n",
    "    ['DOM', 'Dominican Republic', 'Latin America & Caribbean', 'Upper middle income', 243, 'Western Hemisphere', 'EM', 'DO', 'Dominican Republic', 'Dominican Republic'],\n",
    "    ['ECU', 'Ecuador', 'Latin America & Caribbean', 'Upper middle income', 248, 'Western Hemisphere', 'EM', 'EC', 'Ecuador', 'Ecuador'],\n",
    "    ['EGY', 'Egypt, Arab Rep.', 'Middle East & North Africa', 'Lower middle income', 469, 'Middle East and Central Asia', 'EM', 'EG', 'Egypt', 'Egypt'],\n",
    "    ['SLV', 'El Salvador', 'Latin America & Caribbean', 'Upper middle income', 253, 'Western Hemisphere', 'EM', 'SV', 'El Salvador', 'El Salvador'],\n",
    "    ['GNQ', 'Equatorial Guinea', 'Sub-Saharan Africa', 'Upper middle income', 642, 'Africa', 'EM', 'GQ', 'Equatorial Guinea', ''],\n",
    "    ['ERI', 'Eritrea', 'Sub-Saharan Africa', 'Low income', 643, 'Africa', 'LIC', 'ER', 'Eritrea', ''],\n",
    "    ['SWZ', 'Eswatini', 'Sub-Saharan Africa', 'Lower middle income', 734, 'Africa', 'EM', 'SZ', 'Eswatini', ''],\n",
    "    ['ETH', 'Ethiopia', 'Sub-Saharan Africa', 'Low income', 644, 'Africa', 'LIC', 'ET', 'Ethiopia', 'Ethiopia'],\n",
    "    ['FJI', 'Fiji', 'East Asia & Pacific', 'Upper middle income', 819, 'Asia and Pacific', 'EM', 'FJ', 'Fiji', ''],\n",
    "    ['GAB', 'Gabon', 'Sub-Saharan Africa', 'Upper middle income', 646, 'Africa', 'EM', 'GA', 'Gabon', ''],\n",
    "    ['GMB', 'Gambia, The', 'Sub-Saharan Africa', 'Low income', 648, 'Africa', 'LIC', 'GM', 'Gambia', ''],\n",
    "    ['GEO', 'Georgia', 'Europe & Central Asia', 'Upper middle income', 915, 'Middle East and Central Asia', 'EM', 'GE', 'Georgia', 'Georgia'],\n",
    "    ['GHA', 'Ghana', 'Sub-Saharan Africa', 'Lower middle income', 652, 'Africa', 'LIC', 'GH', 'Ghana', 'Ghana'],\n",
    "    ['GRD', 'Grenada', 'Latin America & Caribbean', 'Upper middle income', 328, 'Western Hemisphere', 'LIC', 'GD', 'Grenada', ''],\n",
    "    ['GTM', 'Guatemala', 'Latin America & Caribbean', 'Upper middle income', 258, 'Western Hemisphere', 'EM', 'GT', 'Guatemala', 'Guatemala'],\n",
    "    ['GIN', 'Guinea', 'Sub-Saharan Africa', 'Lower middle income', 656, 'Africa', 'LIC', 'GN', 'Guinea', ''],\n",
    "    ['GNB', 'Guinea-Bissau', 'Sub-Saharan Africa', 'Low income', 654, 'Africa', 'LIC', 'GW', 'Guinea-Bissau', ''],\n",
    "    ['HTI', 'Haiti', 'Latin America & Caribbean', 'Lower middle income', 263, 'Western Hemisphere', 'LIC', 'HT', 'Haiti', ''],\n",
    "    ['HND', 'Honduras', 'Latin America & Caribbean', 'Lower middle income', 268, 'Western Hemisphere', 'LIC', 'HN', 'Honduras', ''],\n",
    "    ['IND', 'India', 'South Asia', 'Lower middle income', 534, 'Asia and Pacific', 'EM', 'IN', 'India', 'India'],\n",
    "    ['IDN', 'Indonesia', 'East Asia & Pacific', 'Upper middle income', 536, 'Asia and Pacific', 'EM', 'ID', 'Indonesia', 'Indonesia'],\n",
    "    ['IRN', 'Iran, Islamic Rep.', 'Middle East & North Africa', 'Lower middle income', 429, 'Middle East and Central Asia', 'EM', 'IR', 'Iran (Islamic Republic of)', ''],\n",
    "    ['IRQ', 'Iraq', 'Middle East & North Africa', 'Upper middle income', 433, '', '', 'IQ', 'Iraq', ''],\n",
    "    ['JAM', 'Jamaica', 'Latin America & Caribbean', 'Upper middle income', 343, 'Western Hemisphere', 'EM', 'JM', 'Jamaica', 'Jamaica'],\n",
    "    ['JOR', 'Jordan', 'Middle East & North Africa', 'Lower middle income', 439, 'Middle East and Central Asia', 'EM', 'JO', 'Jordan', 'Jordan'],\n",
    "    ['KAZ', 'Kazakhstan', 'Europe & Central Asia', 'Upper middle income', 916, 'Middle East and Central Asia', 'EM', 'KZ', 'Kazakhstan', 'Kazakhstan'],\n",
    "    ['KEN', 'Kenya', 'Sub-Saharan Africa', 'Lower middle income', 664, 'Africa', 'LIC', 'KE', 'Kenya', 'Kenya'],\n",
    "    ['KIR', 'Kiribati', 'East Asia & Pacific', 'Lower middle income', 826, 'Asia and Pacific', 'LIC', 'KI', 'Kiribati', ''],\n",
    "    ['PRK', 'Korea, Dem. People\\'s Rep.', 'East Asia & Pacific', 'Low income', '', '', '', 'KP', 'Korea, Republic of', 'Korea, South'],\n",
    "    ['XKX', 'Kosovo', 'Europe & Central Asia', 'Upper middle income', '', '', '', 'XK', '', ''],\n",
    "    ['KGZ', 'Kyrgyz Republic', 'Europe & Central Asia', 'Lower middle income', 917, 'Middle East and Central Asia', 'LIC', 'KG', 'Kyrgyzstan', 'Kyrgyzstan'],\n",
    "    ['LAO', 'Lao PDR', 'East Asia & Pacific', 'Lower middle income', 544, 'Asia and Pacific', 'LIC', 'LA', 'Lao People\\'s Dem. Rep.', ''],\n",
    "    ['LBN', 'Lebanon', 'Middle East & North Africa', 'Lower middle income', 446, 'Middle East and Central Asia', 'EM', 'LB', 'Lebanon', 'Lebanon'],\n",
    "    ['LSO', 'Lesotho', 'Sub-Saharan Africa', 'Lower middle income', 666, 'Africa', 'LIC', 'LS', 'Lesotho', 'Lesotho'],\n",
    "    ['LBR', 'Liberia', 'Sub-Saharan Africa', 'Low income', 668, 'Africa', 'LIC', 'LR', 'Liberia', ''],\n",
    "    ['LBY', 'Libya', 'Middle East & North Africa', 'Upper middle income', 672, 'Middle East and Central Asia', 'EM', 'LY', 'Libya', ''],\n",
    "    ['MDG', 'Madagascar', 'Sub-Saharan Africa', 'Low income', 674, 'Africa', 'LIC', 'MG', 'Madagascar', 'Madagascar'],\n",
    "    ['MWI', 'Malawi', 'Sub-Saharan Africa', 'Low income', 676, 'Africa', 'LIC', 'MW', 'Malawi', 'Malawi'],\n",
    "    ['MYS', 'Malaysia', 'East Asia & Pacific', 'Upper middle income', 548, 'Asia and Pacific', 'EM', 'MY', 'Malaysia', 'Malaysia'],\n",
    "    ['MDV', 'Maldives', 'South Asia', 'Upper middle income', 556, 'Asia and Pacific', 'LIC', 'MV', 'Maldives', ''],\n",
    "    ['MLI', 'Mali', 'Sub-Saharan Africa', 'Low income', 678, 'Africa', 'LIC', 'ML', 'Mali', 'Mali'],\n",
    "    ['MHL', 'Marshall Islands', 'East Asia & Pacific', 'Upper middle income', 867, 'Asia and Pacific', 'LIC', 'MH', 'Marshall Islands', ''],\n",
    "    ['MRT', 'Mauritania', 'Sub-Saharan Africa', 'Lower middle income', 682, 'Middle East and Central Asia', 'LIC', 'MR', 'Mauritania', 'Mauritania'],\n",
    "    ['MUS', 'Mauritius', 'Sub-Saharan Africa', 'Upper middle income', 684, 'Africa', 'EM', 'MU', 'Mauritius', 'Mauritius'],\n",
    "    ['MEX', 'Mexico', 'Latin America & Caribbean', 'Upper middle income', 273, 'Western Hemisphere', 'EM', 'MX', 'Mexico', 'Mexico'],\n",
    "    ['FSM', 'Micronesia, Fed. Sts.', 'East Asia & Pacific', 'Lower middle income', 868, 'Asia and Pacific', 'LIC', 'FM', '', ''],\n",
    "    ['MDA', 'Moldova', 'Europe & Central Asia', 'Upper middle income', 921, 'Europe', 'LIC', 'MD', 'Moldova, Republic of', 'Moldova'],\n",
    "    ['MNG', 'Mongolia', 'East Asia & Pacific', 'Lower middle income', 948, 'Asia and Pacific', 'EM', 'MN', 'Mongolia', 'Mongolia'],\n",
    "    ['MNE', 'Montenegro', 'Europe & Central Asia', 'Upper middle income', 943, '', '', 'ME', 'Montenegro', 'Montenegro'],\n",
    "    ['MAR', 'Morocco', 'Middle East & North Africa', 'Lower middle income', 686, 'Middle East and Central Asia', 'EM', 'MA', 'Morocco', 'Morocco'],\n",
    "    ['MOZ', 'Mozambique', 'Sub-Saharan Africa', 'Low income', 688, 'Africa', 'LIC', 'MZ', 'Mozambique', 'Mozambique'],\n",
    "    ['MMR', 'Myanmar', 'East Asia & Pacific', 'Lower middle income', 518, 'Asia and Pacific', 'LIC', 'MM', 'Myanmar', ''],\n",
    "    ['NAM', 'Namibia', 'Sub-Saharan Africa', 'Upper middle income', 728, 'Africa', 'EM', 'NA', 'Namibia', 'Namibia'],\n",
    "    ['NPL', 'Nepal', 'South Asia', 'Lower middle income', 558, 'Asia and Pacific', 'LIC', 'NP', 'Nepal', ''],\n",
    "    ['NIC', 'Nicaragua', 'Latin America & Caribbean', 'Lower middle income', 278, 'Western Hemisphere', 'LIC', 'NI', 'Nicaragua', 'Nicaragua'],\n",
    "    ['NER', 'Niger', 'Sub-Saharan Africa', 'Low income', 692, 'Africa', 'LIC', 'NE', 'Niger', ''],\n",
    "    ['NGA', 'Nigeria', 'Sub-Saharan Africa', 'Lower middle income', 694, 'Africa', 'EM', 'NG', 'Nigeria', 'Nigeria'],\n",
    "    ['MKD', 'North Macedonia', 'Europe & Central Asia', 'Upper middle income', 962, 'Europe', 'EM', 'MK', 'North Macedonia', ''],\n",
    "    ['PAK', 'Pakistan', 'South Asia', 'Lower middle income', 564, 'Middle East and Central Asia', 'EM', 'PK', 'Pakistan', 'Pakistan'],\n",
    "    ['PLW', 'Palau', 'East Asia & Pacific', 'Upper middle income', 565, '', '', 'PW', 'Palau', ''],\n",
    "    ['PNG', 'Papua New Guinea', 'East Asia & Pacific', 'Lower middle income', 853, 'Asia and Pacific', 'LIC', 'PG', 'Papua New Guinea', ''],\n",
    "    ['PRY', 'Paraguay', 'Latin America & Caribbean', 'Upper middle income', 288, 'Western Hemisphere', 'EM', 'PY', 'Paraguay', 'Paraguay'],\n",
    "    ['PER', 'Peru', 'Latin America & Caribbean', 'Upper middle income', 293, 'Western Hemisphere', 'EM', 'PE', 'Peru', 'Peru'],\n",
    "    ['PHL', 'Philippines', 'East Asia & Pacific', 'Lower middle income', 566, 'Asia and Pacific', 'EM', 'PH', 'Philippines', 'Philippines'],\n",
    "    ['RUS', 'Russian Federation', 'Europe & Central Asia', 'Upper middle income', 922, 'Europe', 'EM', 'RU', 'Russian Federation', 'Russian Federation'],\n",
    "    ['RWA', 'Rwanda', 'Sub-Saharan Africa', 'Low income', 714, 'Africa', 'LIC', 'RW', 'Rwanda', 'Rwanda'],\n",
    "    ['WSM', 'Samoa', 'East Asia & Pacific', 'Lower middle income', 862, 'Asia and Pacific', 'LIC', 'WS', 'Samoa', ''],\n",
    "    ['STP', 'São Tomé and Príncipe', 'Sub-Saharan Africa', 'Lower middle income', 716, 'Africa', 'LIC', 'ST', 'Sao Tome and Principe', ''],\n",
    "    ['SEN', 'Senegal', 'Sub-Saharan Africa', 'Lower middle income', 722, 'Africa', 'LIC', 'SN', 'Senegal', 'Senegal'],\n",
    "    ['SRB', 'Serbia', 'Europe & Central Asia', 'Upper middle income', 942, 'Europe', 'EM', 'RS', 'Serbia', 'Serbia'],\n",
    "    ['SLE', 'Sierra Leone', 'Sub-Saharan Africa', 'Low income', 724, 'Africa', 'LIC', 'SL', 'Sierra Leone', ''],\n",
    "    ['SLB', 'Solomon Islands', 'East Asia & Pacific', 'Lower middle income', 813, 'Asia and Pacific', 'LIC', 'SB', 'Solomon Islands', ''],\n",
    "    ['SOM', 'Somalia', 'Sub-Saharan Africa', 'Low income', 726, '', '', 'SO', 'Somalia', ''],\n",
    "    ['ZAF', 'South Africa', 'Sub-Saharan Africa', 'Upper middle income', 199, 'Africa', 'EM', 'ZA', 'South Africa', 'South Africa'],\n",
    "    ['SSD', 'South Sudan', 'Sub-Saharan Africa', 'Low income', 733, 'Africa', 'LIC', 'SS', 'South Sudan', ''],\n",
    "    ['LKA', 'Sri Lanka', 'South Asia', 'Lower middle income', 524, 'Asia and Pacific', 'EM', 'LK', 'Sri Lanka', 'Sri Lanka'],\n",
    "    ['LCA', 'St. Lucia', 'Latin America & Caribbean', 'Upper middle income', 362, 'Western Hemisphere', 'LIC', 'LC', 'Saint Lucia', ''],\n",
    "    ['VCT', 'St. Vincent and the Grenadines', 'Latin America & Caribbean', 'Upper middle income', 364, 'Western Hemisphere', 'LIC', 'VC', 'Saint Vincent and the Grenadines', ''],\n",
    "    ['SDN', 'Sudan', 'Sub-Saharan Africa', 'Low income', 732, 'Middle East and Central Asia', 'LIC', 'SD', 'Sudan', ''],\n",
    "    ['SUR', 'Suriname', 'Latin America & Caribbean', 'Upper middle income', 366, 'Western Hemisphere', 'EM', 'SR', 'Suriname', ''],\n",
    "    ['SYR', 'Syrian Arab Republic', 'Middle East & North Africa', 'Low income', 463, 'Middle East and Central Asia', 'EM', 'SY', 'Syrian Arab Republic', 'Syria'],\n",
    "    ['TJK', 'Tajikistan', 'Europe & Central Asia', 'Lower middle income', 923, 'Middle East and Central Asia', 'LIC', 'TJ', 'Tajikistan', ''],\n",
    "    ['TZA', 'Tanzania', 'Sub-Saharan Africa', 'Lower middle income', 738, 'Africa', 'LIC', 'TZ', 'Tanzania, United Republic of', 'Tanzania'],\n",
    "    ['THA', 'Thailand', 'East Asia & Pacific', 'Upper middle income', 578, 'Asia and Pacific', 'EM', 'TH', 'Thailand', 'Thailand'],\n",
    "    ['TLS', 'Timor-Leste', 'East Asia & Pacific', 'Lower middle income', 537, 'Asia and Pacific', 'LIC', 'TL', 'Timor-Leste', ''],\n",
    "    ['TGO', 'Togo', 'Sub-Saharan Africa', 'Low income', 742, 'Africa', 'LIC', 'TG', 'Togo', ''],\n",
    "    ['TON', 'Tonga', 'East Asia & Pacific', 'Upper middle income', 866, 'Asia and Pacific', 'LIC', 'TO', 'Tonga', ''],\n",
    "    ['TUN', 'Tunisia', 'Middle East & North Africa', 'Lower middle income', 744, 'Middle East and Central Asia', 'EM', 'TN', 'Tunisia', 'Tunisia'],\n",
    "    ['TUR', 'Türkiye', 'Europe & Central Asia', 'Upper middle income', 186, 'Europe', 'EM', 'TR', 'Turkiye', 'Turkey'],\n",
    "    ['TKM', 'Turkmenistan', 'Europe & Central Asia', 'Upper middle income', 925, 'Middle East and Central Asia', 'EM', 'TM', 'Turkmenistan', ''],\n",
    "    ['TUV', 'Tuvalu', 'East Asia & Pacific', 'Upper middle income', '', '', '', 'TV', 'Tuvalu', ''],\n",
    "    ['UGA', 'Uganda', 'Sub-Saharan Africa', 'Low income', 746, 'Africa', 'LIC', 'UG', 'Uganda', 'Uganda'],\n",
    "    ['UKR', 'Ukraine', 'Europe & Central Asia', 'Lower middle income', 926, 'Europe', 'EM', 'UA', 'Ukraine', 'Ukraine'],\n",
    "    ['UZB', 'Uzbekistan', 'Europe & Central Asia', 'Lower middle income', 927, 'Middle East and Central Asia', 'LIC', 'UZ', 'Uzbekistan', ''],\n",
    "    ['VUT', 'Vanuatu', 'East Asia & Pacific', 'Lower middle income', 846, 'Asia and Pacific', 'LIC', 'VU', 'Vanuatu', ''],\n",
    "    ['VEN', 'Venezuela, RB', 'Latin America & Caribbean', 'Upper middle income', 299, 'Western Hemisphere', 'EM', 'VE', 'Venezuela (Bolivarian Rep. of)', 'Venezuela'],\n",
    "    ['VNM', 'Vietnam', 'East Asia & Pacific', 'Lower middle income', 582, 'Asia and Pacific', 'EM', 'VN', 'Viet Nam', 'Vietnam'],\n",
    "    ['PSE', 'West Bank and Gaza', 'Middle East & North Africa', 'Upper middle income', '', '', '', 'PS', '', ''],\n",
    "    ['YEM', 'Yemen, Rep.', 'Middle East & North Africa', 'Low income', 474, 'Middle East and Central Asia', 'LIC', 'YE', 'Yemen', ''],\n",
    "    ['ZMB', 'Zambia', 'Sub-Saharan Africa', 'Lower middle income', 754, 'Africa', 'LIC', 'ZM', 'Zambia', 'Zambia'],\n",
    "    ['ZWE', 'Zimbabwe', 'Sub-Saharan Africa', 'Lower middle income', 698, '', '', 'ZW', 'Zimbabwe', 'Zimbabwe']\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "columns = [\n",
    "    'code', 'country', 'region', 'incomegroup', 'ifs', 'imf_region', \n",
    "    'imf_income', 'code_2', 'country_unctad', 'country_vcpe'\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "country_code_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Handle numeric columns - convert empty strings to NaN for numeric columns\n",
    "country_code_df['ifs'] = pd.to_numeric(country_code_df['ifs'], errors='coerce')\n",
    "country_code_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GFDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldBankAnalyzer:\n",
    "    def __init__(self, country_codes_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize with country codes DataFrame.\n",
    "        Expected columns: code_2, code, region, incomegroup\n",
    "        \"\"\"\n",
    "        self.country_codes = country_codes_df\n",
    "        self.base_url = \"https://api.worldbank.org/v2/country/all/indicator/\"\n",
    "    \n",
    "    def load_indicator(self, indicator_code: str, column_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and clean a single indicator from World Bank API\"\"\"\n",
    "        url = f\"{self.base_url}{indicator_code}\"\n",
    "        params = {\n",
    "            'format': 'json',\n",
    "            'mrnev': '1',\n",
    "            'per_page': '1000'\n",
    "        }\n",
    "        headers = {'Accept': 'application/json'}\n",
    "        \n",
    "        try:\n",
    "            print(f\"Loading indicator: {indicator_code}\")\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract the data (second element of the response)\n",
    "            records = data[1] if len(data) > 1 else []\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df_records = []\n",
    "            for record in records:\n",
    "                if record.get('value') is not None:  # Only include non-null values\n",
    "                    df_records.append({\n",
    "                        'Country Code': record['country']['id'],\n",
    "                        f'Year_{column_name}': record['date'],\n",
    "                        column_name: record['value']\n",
    "                    })\n",
    "            \n",
    "            return pd.DataFrame(df_records)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading indicator {indicator_code}: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_all_indicators(self) -> pd.DataFrame:\n",
    "        \"\"\"Load all financial indicators\"\"\"\n",
    "        indicators = {\n",
    "            \"NY.GDS.TOTL.ZS\": \"Savings\",\n",
    "            \"GFDD.SI.04\": \"Return on Assets\", \n",
    "            \"GFDD.DI.14\": \"Lending-Deposit Spread\",\n",
    "            \"GFDD.DI.08\": \"Net Interest Margin\",\n",
    "            \"GFDD.SI.01\": \"Z-Score\",\n",
    "            \"GFDD.SI.02\": \"Capital Ratio\"\n",
    "        }\n",
    "        \n",
    "        # Load first indicator as base\n",
    "        first_code, first_name = next(iter(indicators.items()))\n",
    "        result = self.load_indicator(first_code, first_name)\n",
    "        \n",
    "        # Merge with other indicators\n",
    "        for code, name in list(indicators.items())[1:]:\n",
    "            indicator_df = self.load_indicator(code, name)\n",
    "            if not indicator_df.empty:\n",
    "                result = result.merge(\n",
    "                    indicator_df, \n",
    "                    on='Country Code', \n",
    "                    how='left',\n",
    "                    suffixes=('', '_y')\n",
    "                )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def apply_normalizations(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply global, regional, and income-group normalizations\"\"\"\n",
    "        # Join with country metadata\n",
    "        df_with_meta = df.merge(\n",
    "            self.country_codes,\n",
    "            left_on='Country Code',\n",
    "            right_on='code_2',\n",
    "            how='right'\n",
    "        )\n",
    "        \n",
    "        # Define numeric indicator columns\n",
    "        indicators = [\n",
    "            \"Savings\", \"Return on Assets\", \"Lending-Deposit Spread\",\n",
    "            \"Net Interest Margin\", \"Z-Score\", \"Capital Ratio\"\n",
    "        ]\n",
    "        \n",
    "        # 1. Global min-max normalization (suffix _w)\n",
    "        for indicator in indicators:\n",
    "            if indicator in df_with_meta.columns:\n",
    "                values = df_with_meta[indicator].dropna()\n",
    "                if len(values) > 0:\n",
    "                    min_val = values.min()\n",
    "                    max_val = values.max()\n",
    "                    if min_val != max_val:\n",
    "                        df_with_meta[f'{indicator}_w'] = (\n",
    "                            df_with_meta[indicator] - min_val\n",
    "                        ) / (max_val - min_val)\n",
    "                    else:\n",
    "                        df_with_meta[f'{indicator}_w'] = None\n",
    "        \n",
    "        # 2. Regional min-max normalization (suffix _r)\n",
    "        for indicator in indicators:\n",
    "            if indicator in df_with_meta.columns:\n",
    "                regional_bounds = df_with_meta.groupby('region')[indicator].agg(['min', 'max'])\n",
    "                \n",
    "                def regional_normalize(row):\n",
    "                    if pd.isna(row[indicator]) or pd.isna(row['region']):\n",
    "                        return None\n",
    "                    region = row['region']\n",
    "                    if region in regional_bounds.index:\n",
    "                        min_val = regional_bounds.loc[region, 'min']\n",
    "                        max_val = regional_bounds.loc[region, 'max']\n",
    "                        if pd.notna(min_val) and pd.notna(max_val) and min_val != max_val:\n",
    "                            return (row[indicator] - min_val) / (max_val - min_val)\n",
    "                    return None\n",
    "                \n",
    "                df_with_meta[f'{indicator}_r'] = df_with_meta.apply(regional_normalize, axis=1)\n",
    "        \n",
    "        # 3. Income-group min-max normalization (suffix _i)\n",
    "        for indicator in indicators:\n",
    "            if indicator in df_with_meta.columns:\n",
    "                income_bounds = df_with_meta.groupby('incomegroup')[indicator].agg(['min', 'max'])\n",
    "                \n",
    "                def income_normalize(row):\n",
    "                    if pd.isna(row[indicator]) or pd.isna(row['incomegroup']):\n",
    "                        return None\n",
    "                    income_group = row['incomegroup']\n",
    "                    if income_group in income_bounds.index:\n",
    "                        min_val = income_bounds.loc[income_group, 'min']\n",
    "                        max_val = income_bounds.loc[income_group, 'max']\n",
    "                        if pd.notna(min_val) and pd.notna(max_val) and min_val != max_val:\n",
    "                            return (row[indicator] - min_val) / (max_val - min_val)\n",
    "                    return None\n",
    "                \n",
    "                df_with_meta[f'{indicator}_i'] = df_with_meta.apply(income_normalize, axis=1)\n",
    "        \n",
    "        return df_with_meta\n",
    "    \n",
    "    def process_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Main processing function\"\"\"\n",
    "        # Load all indicators\n",
    "        print(\"Loading indicators from World Bank API...\")\n",
    "        df = self.load_all_indicators()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No data loaded\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Apply normalizations\n",
    "        print(\"Applying normalizations...\")\n",
    "        df_normalized = self.apply_normalizations(df)\n",
    "        \n",
    "        # Remove unnecessary columns\n",
    "        columns_to_remove = ['Country Code', 'region', 'incomegroup', 'code_2']\n",
    "        df_final = df_normalized.drop(columns=[col for col in columns_to_remove if col in df_normalized.columns])\n",
    "        \n",
    "        # Reorder columns (similar to PowerQuery)\n",
    "        base_columns = ['code']\n",
    "        indicator_columns = []\n",
    "        \n",
    "        indicators = [\"Savings\", \"Return on Assets\", \"Lending-Deposit Spread\", \n",
    "                     \"Net Interest Margin\", \"Z-Score\", \"Capital Ratio\"]\n",
    "        \n",
    "        for indicator in indicators:\n",
    "            if f'Year_{indicator}' in df_final.columns:\n",
    "                indicator_columns.append(f'Year_{indicator}')\n",
    "            if indicator in df_final.columns:\n",
    "                indicator_columns.append(indicator)\n",
    "        \n",
    "        # Add normalized columns\n",
    "        for suffix in ['_w', '_r', '_i']:\n",
    "            for indicator in indicators:\n",
    "                col_name = f'{indicator}{suffix}'\n",
    "                if col_name in df_final.columns:\n",
    "                    indicator_columns.append(col_name)\n",
    "        \n",
    "        # Reorder columns\n",
    "        final_columns = base_columns + indicator_columns\n",
    "        existing_columns = [col for col in final_columns if col in df_final.columns]\n",
    "        df_final = df_final[existing_columns]\n",
    "        \n",
    "        # Sort by country code\n",
    "        df_final = df_final.sort_values('code').reset_index(drop=True)\n",
    "        \n",
    "        return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading indicators from World Bank API...\n",
      "Loading indicator: NY.GDS.TOTL.ZS\n",
      "Loading indicator: GFDD.SI.04\n",
      "Loading indicator: GFDD.DI.14\n",
      "Loading indicator: GFDD.DI.08\n",
      "Loading indicator: GFDD.SI.01\n",
      "Loading indicator: GFDD.SI.02\n",
      "Applying normalizations...\n",
      "Final dataset shape: (135, 31)\n",
      "\n",
      "Columns: ['code', 'Year_Savings', 'Savings', 'Year_Return on Assets', 'Return on Assets', 'Year_Lending-Deposit Spread', 'Lending-Deposit Spread', 'Year_Net Interest Margin', 'Net Interest Margin', 'Year_Z-Score', 'Z-Score', 'Year_Capital Ratio', 'Capital Ratio', 'Savings_w', 'Return on Assets_w', 'Lending-Deposit Spread_w', 'Net Interest Margin_w', 'Z-Score_w', 'Capital Ratio_w', 'Savings_r', 'Return on Assets_r', 'Lending-Deposit Spread_r', 'Net Interest Margin_r', 'Z-Score_r', 'Capital Ratio_r', 'Savings_i', 'Return on Assets_i', 'Lending-Deposit Spread_i', 'Net Interest Margin_i', 'Z-Score_i', 'Capital Ratio_i']\n",
      "\n",
      "First few rows:\n",
      "  code Year_Savings    Savings Year_Return on Assets  Return on Assets  \\\n",
      "0  AFG         2023 -18.571747                  2020          16.83070   \n",
      "1  AGO         2023  38.465120                  2021          37.19412   \n",
      "2  ALB         2023  17.747193                  2021          51.59088   \n",
      "3  ARG         2023  17.663350                  2017          70.41665   \n",
      "4  ARM         2023  20.484902                  2021         131.77010   \n",
      "\n",
      "  Year_Lending-Deposit Spread  Lending-Deposit Spread  \\\n",
      "0                        2020                 3.07194   \n",
      "1                        2020                12.85610   \n",
      "2                        2020                37.99130   \n",
      "3                        2017                15.95850   \n",
      "4                        2020                72.19570   \n",
      "\n",
      "  Year_Net Interest Margin  Net Interest Margin Year_Z-Score  ...  \\\n",
      "0                     2020             17.64849         2021  ...   \n",
      "1                     2021             22.58882         2021  ...   \n",
      "2                     2021             64.32858         2021  ...   \n",
      "3                     2017             21.90973         2021  ...   \n",
      "4                     2021             44.77929         2021  ...   \n",
      "\n",
      "   Lending-Deposit Spread_r Net Interest Margin_r  Z-Score_r  Capital Ratio_r  \\\n",
      "0                  0.000000              0.000000   0.612278              NaN   \n",
      "1                  0.099305              0.096592   0.451448              NaN   \n",
      "2                  0.373395              0.824416   0.427503         0.197685   \n",
      "3                  0.117420              0.000000   0.168522         0.297368   \n",
      "4                  0.885098              0.542649   0.248680         0.159653   \n",
      "\n",
      "   Savings_i  Return on Assets_i  Lending-Deposit Spread_i  \\\n",
      "0   0.261759            0.043147                  0.022350   \n",
      "1   0.964794            0.022312                  0.048095   \n",
      "2   0.406334            0.087439                  0.146790   \n",
      "3   0.405521            0.142080                  0.017035   \n",
      "4   0.432877            0.320157                  0.348227   \n",
      "\n",
      "   Net Interest Margin_i  Z-Score_i  Capital Ratio_i  \n",
      "0               0.183030   0.878088              NaN  \n",
      "1               0.063555   0.230713              NaN  \n",
      "2               0.408497   0.427503         0.166054  \n",
      "3               0.091518   0.162788         0.078915  \n",
      "4               0.262413   0.248680         0.134107  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = WorldBankAnalyzer(country_code_df)\n",
    "\n",
    "# Process data\n",
    "result = analyzer.process_data()\n",
    "\n",
    "\n",
    "print(f\"Final dataset shape: {result.shape}\")\n",
    "print(\"\\nColumns:\", result.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result.head())\n",
    "\n",
    "# Save to CSV\n",
    "result.to_csv('gfdd.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFDD_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldBankChangeAnalyzer:\n",
    "    def __init__(self, country_codes_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize with country codes DataFrame.\n",
    "        Expected columns: code_2, code, region, incomegroup\n",
    "        \"\"\"\n",
    "        self.country_codes = country_codes_df\n",
    "        self.base_url = \"https://api.worldbank.org/v2/country/all/indicator/\"\n",
    "    \n",
    "    def load_indicator(self, indicator_code: str, column_name: str, page: str = \"1\") -> pd.DataFrame:\n",
    "        \"\"\"Load and calculate change over time for a single indicator\"\"\"\n",
    "        url = f\"{self.base_url}{indicator_code}\"\n",
    "        params = {\n",
    "            'format': 'json',\n",
    "            'mrnev': '5',  # Most recent 5 years with data\n",
    "            'per_page': '1000',\n",
    "            'page': page\n",
    "        }\n",
    "        headers = {'Accept': 'application/json'}\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract the data (second element of the response)\n",
    "            records = data[1] if len(data) > 1 else []\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df_records = []\n",
    "            for record in records:\n",
    "                if record.get('value') is not None:  # Only include non-null values\n",
    "                    df_records.append({\n",
    "                        'Country Code': record['country']['id'],\n",
    "                        'Year': int(record['date']),\n",
    "                        column_name: float(record['value'])\n",
    "                    })\n",
    "            \n",
    "            if not df_records:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = pd.DataFrame(df_records)\n",
    "            \n",
    "            # Sort by country and year\n",
    "            df = df.sort_values(['Country Code', 'Year'])\n",
    "            \n",
    "            # Group by country and calculate change over time\n",
    "            def calculate_change(group):\n",
    "                if len(group) < 2:\n",
    "                    return pd.Series({\n",
    "                        'Country Code': group['Country Code'].iloc[0],\n",
    "                        f'd_{column_name}': None,\n",
    "                        f'Year Range_{column_name}': None\n",
    "                    })\n",
    "                \n",
    "                first_year = group['Year'].min()\n",
    "                last_year = group['Year'].max()\n",
    "                start_val = group[group['Year'] == first_year][column_name].iloc[0]\n",
    "                end_val = group[group['Year'] == last_year][column_name].iloc[0]\n",
    "                \n",
    "                # Calculate annual rate of change\n",
    "                if last_year != first_year and pd.notna(start_val) and pd.notna(end_val):\n",
    "                    change_rate = (end_val - start_val) / (last_year - first_year)\n",
    "                else:\n",
    "                    change_rate = None\n",
    "                \n",
    "                year_range = f\"{first_year}–{last_year}\"\n",
    "                \n",
    "                return pd.Series({\n",
    "                    'Country Code': group['Country Code'].iloc[0],\n",
    "                    f'd_{column_name}': change_rate,\n",
    "                    f'Year Range_{column_name}': year_range\n",
    "                })\n",
    "            \n",
    "            result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading indicator {indicator_code}: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_all_indicators(self) -> pd.DataFrame:\n",
    "        \"\"\"Load all financial indicators and calculate changes\"\"\"\n",
    "        indicators = {\n",
    "            \"NY.GDS.TOTL.ZS\": \"Savings\",\n",
    "            \"GFDD.SI.04\": \"Return on Assets\", \n",
    "            \"GFDD.DI.14\": \"Lending-Deposit Spread\",\n",
    "            \"GFDD.DI.08\": \"Net Interest Margin\",\n",
    "            \"GFDD.SI.01\": \"Z-Score\",\n",
    "            \"GFDD.SI.02\": \"Capital Ratio\"\n",
    "        }\n",
    "        \n",
    "        print(\"Loading indicators and calculating changes over time...\")\n",
    "        \n",
    "        # Load first indicator as base\n",
    "        first_code, first_name = next(iter(indicators.items()))\n",
    "        result = self.load_indicator(first_code, first_name, \"1\")\n",
    "        \n",
    "        if result.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Merge with other indicators\n",
    "        for code, name in list(indicators.items())[1:]:\n",
    "            print(f\"Loading {name}...\")\n",
    "            indicator_df = self.load_indicator(code, name, \"1\")\n",
    "            if not indicator_df.empty:\n",
    "                result = result.merge(\n",
    "                    indicator_df, \n",
    "                    on='Country Code', \n",
    "                    how='left'\n",
    "                )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def apply_normalizations(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply global, regional, and income-group normalizations\"\"\"\n",
    "        # Join with country metadata\n",
    "        df_with_meta = df.merge(\n",
    "            self.country_codes,\n",
    "            left_on='Country Code',\n",
    "            right_on='code_2',\n",
    "            how='right'\n",
    "        )\n",
    "        \n",
    "        # Define numeric indicator columns (change rates)\n",
    "        indicators = [\n",
    "            \"d_Savings\", \"d_Return on Assets\", \"d_Lending-Deposit Spread\",\n",
    "            \"d_Net Interest Margin\", \"d_Z-Score\", \"d_Capital Ratio\"\n",
    "        ]\n",
    "        \n",
    "        # Convert to numeric types\n",
    "        for indicator in indicators:\n",
    "            if indicator in df_with_meta.columns:\n",
    "                df_with_meta[indicator] = pd.to_numeric(df_with_meta[indicator], errors='coerce')\n",
    "        \n",
    "        # 1. Global min-max normalization (suffix _w)\n",
    "        for indicator in indicators:\n",
    "            if indicator in df_with_meta.columns:\n",
    "                values = df_with_meta[indicator].dropna()\n",
    "                if len(values) > 0:\n",
    "                    min_val = values.min()\n",
    "                    max_val = values.max()\n",
    "                    if min_val != max_val and pd.notna(min_val) and pd.notna(max_val):\n",
    "                        df_with_meta[f'{indicator}_w'] = (\n",
    "                            df_with_meta[indicator] - min_val\n",
    "                        ) / (max_val - min_val)\n",
    "                    else:\n",
    "                        df_with_meta[f'{indicator}_w'] = None\n",
    "        \n",
    "        # 2. Regional min-max normalization (suffix _r)\n",
    "        for indicator in indicators:\n",
    "            if indicator in df_with_meta.columns:\n",
    "                regional_bounds = df_with_meta.groupby('region')[indicator].agg(['min', 'max'])\n",
    "                \n",
    "                def regional_normalize(row):\n",
    "                    if pd.isna(row[indicator]) or pd.isna(row['region']):\n",
    "                        return None\n",
    "                    region = row['region']\n",
    "                    if region in regional_bounds.index:\n",
    "                        min_val = regional_bounds.loc[region, 'min']\n",
    "                        max_val = regional_bounds.loc[region, 'max']\n",
    "                        if pd.notna(min_val) and pd.notna(max_val) and min_val != max_val:\n",
    "                            return (row[indicator] - min_val) / (max_val - min_val)\n",
    "                    return None\n",
    "                \n",
    "                df_with_meta[f'{indicator}_r'] = df_with_meta.apply(regional_normalize, axis=1)\n",
    "        \n",
    "        # 3. Income-group min-max normalization (suffix _i)\n",
    "        for indicator in indicators:\n",
    "            if indicator in df_with_meta.columns:\n",
    "                income_bounds = df_with_meta.groupby('incomegroup')[indicator].agg(['min', 'max'])\n",
    "                \n",
    "                def income_normalize(row):\n",
    "                    if pd.isna(row[indicator]) or pd.isna(row['incomegroup']):\n",
    "                        return None\n",
    "                    income_group = row['incomegroup']\n",
    "                    if income_group in income_bounds.index:\n",
    "                        min_val = income_bounds.loc[income_group, 'min']\n",
    "                        max_val = income_bounds.loc[income_group, 'max']\n",
    "                        if pd.notna(min_val) and pd.notna(max_val) and min_val != max_val:\n",
    "                            return (row[indicator] - min_val) / (max_val - min_val)\n",
    "                    return None\n",
    "                \n",
    "                df_with_meta[f'{indicator}_i'] = df_with_meta.apply(income_normalize, axis=1)\n",
    "        \n",
    "        return df_with_meta\n",
    "    \n",
    "    def process_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Main processing function\"\"\"\n",
    "        # Load all indicators and calculate changes\n",
    "        df = self.load_all_indicators()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No data loaded\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Loaded data for {len(df)} countries\")\n",
    "        \n",
    "        # Apply normalizations\n",
    "        print(\"Applying normalizations...\")\n",
    "        df_normalized = self.apply_normalizations(df)\n",
    "        \n",
    "        # Remove unnecessary columns\n",
    "        columns_to_remove = ['Country Code', 'region', 'incomegroup', 'code_2']\n",
    "        df_final = df_normalized.drop(columns=[col for col in columns_to_remove if col in df_normalized.columns])\n",
    "        \n",
    "        # Reorder columns (similar to PowerQuery)\n",
    "        base_columns = ['code']\n",
    "        \n",
    "        # Define indicators for column ordering\n",
    "        indicator_names = [\"Savings\", \"Return on Assets\", \"Lending-Deposit Spread\", \n",
    "                          \"Net Interest Margin\", \"Z-Score\", \"Capital Ratio\"]\n",
    "        \n",
    "        # Build column order\n",
    "        ordered_columns = base_columns.copy()\n",
    "        \n",
    "        # Add change and year range columns\n",
    "        for indicator in indicator_names:\n",
    "            change_col = f'd_{indicator}'\n",
    "            year_col = f'Year Range_{indicator}'\n",
    "            if change_col in df_final.columns:\n",
    "                ordered_columns.append(change_col)\n",
    "            if year_col in df_final.columns:\n",
    "                ordered_columns.append(year_col)\n",
    "        \n",
    "        # Add normalized columns\n",
    "        for suffix in ['_w', '_r', '_i']:\n",
    "            for indicator in indicator_names:\n",
    "                col_name = f'd_{indicator}{suffix}'\n",
    "                if col_name in df_final.columns:\n",
    "                    ordered_columns.append(col_name)\n",
    "        \n",
    "        # Select and reorder existing columns\n",
    "        existing_columns = [col for col in ordered_columns if col in df_final.columns]\n",
    "        df_final = df_final[existing_columns]\n",
    "        \n",
    "        # Sort by country code\n",
    "        df_final = df_final.sort_values('code').reset_index(drop=True)\n",
    "        \n",
    "        return df_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading indicators and calculating changes over time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3548428680.py:75: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Return on Assets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3548428680.py:75: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Lending-Deposit Spread...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3548428680.py:75: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Net Interest Margin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3548428680.py:75: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Z-Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3548428680.py:75: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Capital Ratio...\n",
      "Loaded data for 201 countries\n",
      "Applying normalizations...\n",
      "\n",
      "Columns: ['code', 'd_Savings', 'Year Range_Savings', 'd_Return on Assets', 'Year Range_Return on Assets', 'd_Lending-Deposit Spread', 'Year Range_Lending-Deposit Spread', 'd_Net Interest Margin', 'Year Range_Net Interest Margin', 'd_Z-Score', 'Year Range_Z-Score', 'd_Capital Ratio', 'Year Range_Capital Ratio', 'd_Savings_w', 'd_Return on Assets_w', 'd_Lending-Deposit Spread_w', 'd_Net Interest Margin_w', 'd_Z-Score_w', 'd_Capital Ratio_w', 'd_Savings_r', 'd_Return on Assets_r', 'd_Lending-Deposit Spread_r', 'd_Net Interest Margin_r', 'd_Z-Score_r', 'd_Capital Ratio_r', 'd_Savings_i', 'd_Return on Assets_i', 'd_Lending-Deposit Spread_i', 'd_Net Interest Margin_i', 'd_Z-Score_i', 'd_Capital Ratio_i']\n",
      "\n",
      "First few rows:\n",
      "  code  d_Savings Year Range_Savings  d_Return on Assets  \\\n",
      "0  AFG  -1.386393          2020–2023           -0.769712   \n",
      "1  AGO  -0.749778          2019–2023           -3.923900   \n",
      "2  ALB   1.492386          2019–2023            0.246382   \n",
      "3  ARG   0.058544          2019–2023           -1.938112   \n",
      "4  ARM   4.122231          2019–2023            0.747500   \n",
      "\n",
      "  Year Range_Return on Assets  d_Lending-Deposit Spread  \\\n",
      "0                   2016–2020                 -0.204095   \n",
      "1                   2017–2021                 -2.060575   \n",
      "2                   2017–2021                  0.348075   \n",
      "3                   2013–2017                  0.057350   \n",
      "4                   2017–2021                  5.830025   \n",
      "\n",
      "  Year Range_Lending-Deposit Spread  d_Net Interest Margin  \\\n",
      "0                         2016–2020              -0.329075   \n",
      "1                         2016–2020              -1.877890   \n",
      "2                         2016–2020              -0.004335   \n",
      "3                         2013–2017               0.602627   \n",
      "4                         2016–2020               1.713078   \n",
      "\n",
      "  Year Range_Net Interest Margin  d_Z-Score  ... d_Lending-Deposit Spread_r  \\\n",
      "0                      2016–2020   1.795365  ...                   0.000000   \n",
      "1                      2017–2021   1.643132  ...                   0.098460   \n",
      "2                      2017–2021  -0.662155  ...                   0.286091   \n",
      "3                      2013–2017   0.394323  ...                   0.265626   \n",
      "4                      2017–2021  -0.642300  ...                   1.000000   \n",
      "\n",
      "   d_Net Interest Margin_r d_Z-Score_r  d_Capital Ratio_r  d_Savings_i  \\\n",
      "0                 0.155729    1.000000                NaN     0.613993   \n",
      "1                 0.048788    0.916773                NaN     0.698766   \n",
      "2                 0.311421    0.282018           0.000000     0.682499   \n",
      "3                 0.555950    0.702429           1.000000     0.516531   \n",
      "4                 0.681053    0.298443           0.847162     0.986903   \n",
      "\n",
      "   d_Return on Assets_i  d_Lending-Deposit Spread_i  d_Net Interest Margin_i  \\\n",
      "0              0.520121                    0.154284                 0.291904   \n",
      "1              0.162605                    0.042207                 0.003874   \n",
      "2              0.496315                    0.294088                 0.789054   \n",
      "3              0.448811                    0.269703                 0.802547   \n",
      "4              0.507212                    0.753878                 0.827231   \n",
      "\n",
      "   d_Z-Score_i  d_Capital Ratio_i  \n",
      "0     1.000000                NaN  \n",
      "1     0.916723                NaN  \n",
      "2     0.145524           0.000000  \n",
      "3     0.325286           0.288849  \n",
      "4     0.148902           0.236823  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "Change rate statistics:\n",
      "d_Savings: 101 countries, mean change: -0.3623\n",
      "d_Return on Assets: 97 countries, mean change: -1.0946\n",
      "d_Lending-Deposit Spread: 98 countries, mean change: 1.4369\n",
      "d_Net Interest Margin: 96 countries, mean change: 1.4168\n",
      "d_Z-Score: 86 countries, mean change: 0.0546\n",
      "d_Capital Ratio: 70 countries, mean change: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3548428680.py:75: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample country codes DataFrame - replace with your actual data\n",
    "# Initialize analyzer\n",
    "analyzer = WorldBankChangeAnalyzer(country_code_df)\n",
    "\n",
    "# Process data\n",
    "result = analyzer.process_data()\n",
    "\n",
    "if not result.empty:\n",
    "    print(\"\\nColumns:\", result.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(result.head())\n",
    "    \n",
    "    # Show some statistics about the change rates\n",
    "    change_columns = [col for col in result.columns if col.startswith('d_') and not col.endswith(('_w', '_r', '_i'))]\n",
    "    print(f\"\\nChange rate statistics:\")\n",
    "    for col in change_columns:\n",
    "        if col in result.columns:\n",
    "            non_null_count = result[col].notna().sum()\n",
    "            mean_change = result[col].mean()\n",
    "            print(f\"{col}: {non_null_count} countries, mean change: {mean_change:.4f}\")\n",
    "\n",
    "# Save to CSV\n",
    "result.to_csv('gfdd_d.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IC_FRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicator(indicator_code: str, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and clean a single World Bank indicator\n",
    "    \"\"\"\n",
    "    url = f\"https://api.worldbank.org/v2/country/all/indicator/{indicator_code}\"\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'mrnev': '1',\n",
    "        'per_page': '1000'\n",
    "    }\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract the actual data (second element in the response)\n",
    "        if len(data) > 1 and data[1]:\n",
    "            records = data[1]\n",
    "        else:\n",
    "            print(f\"No data found for indicator {indicator_code}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df_list = []\n",
    "        for record in records:\n",
    "            if record.get('value') is not None:\n",
    "                row = {\n",
    "                    'Country Code': record['country']['id'],\n",
    "                    f'Year_{column_name}': record['date'],\n",
    "                    column_name: record['value']\n",
    "                }\n",
    "                df_list.append(row)\n",
    "        \n",
    "        return pd.DataFrame(df_list)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading indicator {indicator_code}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_country_codes() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load country codes mapping (you'll need to provide this data)\n",
    "    This is a placeholder - replace with your actual country codes data\n",
    "    \"\"\"\n",
    "    # This would typically come from your Country_codes table\n",
    "    # For now, creating a basic structure - you'll need to replace this\n",
    "    return pd.DataFrame({\n",
    "        'code_2': ['US', 'GB', 'FR', 'DE', 'JP'],  # Example codes\n",
    "        'code': ['USA', 'GBR', 'FRA', 'DEU', 'JPN'],\n",
    "        'region': ['North America', 'Europe', 'Europe', 'Europe', 'Asia'],\n",
    "        'incomegroup': ['High income', 'High income', 'High income', 'High income', 'High income']\n",
    "    })\n",
    "\n",
    "def normalize_globally(df: pd.DataFrame, indicators: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply global min-max normalization (suffix _w)\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    for indicator in indicators:\n",
    "        if indicator in df_norm.columns:\n",
    "            values = df_norm[indicator].dropna()\n",
    "            if len(values) > 0:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                \n",
    "                if min_val != max_val:\n",
    "                    df_norm[f'{indicator}_w'] = (df_norm[indicator] - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    df_norm[f'{indicator}_w'] = None\n",
    "            else:\n",
    "                df_norm[f'{indicator}_w'] = None\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "def normalize_by_group(df: pd.DataFrame, indicators: List[str], group_col: str, suffix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply group-based min-max normalization\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    for indicator in indicators:\n",
    "        if indicator in df_norm.columns:\n",
    "            # Calculate min/max by group\n",
    "            group_stats = df_norm.groupby(group_col)[indicator].agg(['min', 'max']).reset_index()\n",
    "            \n",
    "            # Merge back to main dataframe\n",
    "            df_norm = df_norm.merge(group_stats, on=group_col, how='left', suffixes=('', '_temp'))\n",
    "            \n",
    "            # Apply normalization\n",
    "            mask = (df_norm['min'] != df_norm['max']) & df_norm['min'].notna() & df_norm['max'].notna()\n",
    "            df_norm[f'{indicator}_{suffix}'] = None\n",
    "            df_norm.loc[mask, f'{indicator}_{suffix}'] = (\n",
    "                (df_norm.loc[mask, indicator] - df_norm.loc[mask, 'min']) / \n",
    "                (df_norm.loc[mask, 'max'] - df_norm.loc[mask, 'min'])\n",
    "            )\n",
    "            \n",
    "            # Clean up temporary columns\n",
    "            df_norm = df_norm.drop(['min', 'max'], axis=1)\n",
    "    \n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting World Bank data processing...\n",
      "Loading FinAccess...\n",
      "Loading FinConstraint...\n",
      "Loading FinLine...\n",
      "Loading FinLoan...\n",
      "Loading Corruption...\n",
      "Loading CrimeLoss...\n",
      "Loading SecurityFirms...\n",
      "Loading SecurityCost...\n",
      "Merging FinConstraint...\n",
      "Merging FinLine...\n",
      "Merging FinLoan...\n",
      "Merging Corruption...\n",
      "Merging CrimeLoss...\n",
      "Merging SecurityFirms...\n",
      "Merging SecurityCost...\n",
      "Loading country codes...\n",
      "Applying global normalization...\n",
      "Applying regional normalization...\n",
      "Applying income group normalization...\n",
      "Data saved to ic_frm.csv\n",
      "Final dataset shape: (5, 41)\n",
      "Columns: ['code', 'Year_FinAccess', 'FinAccess', 'Year_FinConstraint', 'FinConstraint', 'Year_FinLine', 'FinLine', 'Year_FinLoan', 'FinLoan', 'Year_Corruption', 'Corruption', 'Year_CrimeLoss', 'CrimeLoss', 'Year_SecurityFirms', 'SecurityFirms', 'Year_SecurityCost', 'SecurityCost', 'FinAccess_w', 'FinConstraint_w', 'FinLine_w', 'FinLoan_w', 'Corruption_w', 'CrimeLoss_w', 'SecurityFirms_w', 'SecurityCost_w', 'FinAccess_r', 'FinConstraint_r', 'FinLine_r', 'FinLoan_r', 'Corruption_r', 'CrimeLoss_r', 'SecurityFirms_r', 'SecurityCost_r', 'FinAccess_i', 'FinConstraint_i', 'FinLine_i', 'FinLoan_i', 'Corruption_i', 'CrimeLoss_i', 'SecurityFirms_i', 'SecurityCost_i']\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting World Bank data processing...\")\n",
    "\n",
    "# Define indicators\n",
    "indicators_config = {\n",
    "    \"IC.FRM.FIN.FIN12\": \"FinAccess\",\n",
    "    \"IC.FRM.FIN.FIN16\": \"FinConstraint\", \n",
    "    \"IC.FRM.FIN.FIN20\": \"FinLine\",\n",
    "    \"IC.FRM.FIN.FIN21\": \"FinLoan\",\n",
    "    \"IC.FRM.CORR.CORR11\": \"Corruption\",\n",
    "    \"IC.FRM.CRM.CRIME1\": \"CrimeLoss\",\n",
    "    \"IC.FRM.CRM.CRIME8\": \"SecurityFirms\",\n",
    "    \"IC.FRM.CRM.CRIME10\": \"SecurityCost\"\n",
    "}\n",
    "\n",
    "# Load all indicators\n",
    "dataframes = {}\n",
    "for code, name in indicators_config.items():\n",
    "    print(f\"Loading {name}...\")\n",
    "    df = load_indicator(code, name)\n",
    "    if not df.empty:\n",
    "        dataframes[name] = df\n",
    "    else:\n",
    "        print(f\"Warning: No data loaded for {name}\")\n",
    "\n",
    "if not dataframes:\n",
    "    print(\"No data loaded. Exiting.\")\n",
    "\n",
    "# Start with the first dataframe\n",
    "first_key = list(dataframes.keys())[0]\n",
    "merged_df = dataframes[first_key].copy()\n",
    "\n",
    "# Merge all indicators step by step\n",
    "for name in list(dataframes.keys())[1:]:\n",
    "    if name in dataframes:\n",
    "        print(f\"Merging {name}...\")\n",
    "        merged_df = merged_df.merge(\n",
    "            dataframes[name], \n",
    "            on='Country Code', \n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "print(\"Loading country codes...\")\n",
    "country_codes = load_country_codes()\n",
    "\n",
    "# Join with country codes (right outer join equivalent)\n",
    "final_df = country_codes.merge(\n",
    "    merged_df,\n",
    "    left_on='code_2',\n",
    "    right_on='Country Code',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# List of numeric indicators for normalization\n",
    "indicators = list(indicators_config.values())\n",
    "\n",
    "print(\"Applying global normalization...\")\n",
    "final_df = normalize_globally(final_df, indicators)\n",
    "\n",
    "print(\"Applying regional normalization...\")\n",
    "final_df = normalize_by_group(final_df, indicators, 'region', 'r')\n",
    "\n",
    "print(\"Applying income group normalization...\")\n",
    "final_df = normalize_by_group(final_df, indicators, 'incomegroup', 'i')\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_remove = ['region', 'incomegroup', 'Country Code']\n",
    "final_df = final_df.drop([col for col in columns_to_remove if col in final_df.columns], axis=1)\n",
    "\n",
    "# Reorder columns as in the original query\n",
    "base_columns = ['code']\n",
    "year_value_pairs = []\n",
    "normalized_w = []\n",
    "normalized_r = []\n",
    "normalized_i = []\n",
    "\n",
    "for indicator in indicators:\n",
    "    year_col = f'Year_{indicator}'\n",
    "    if year_col in final_df.columns:\n",
    "        year_value_pairs.extend([year_col, indicator])\n",
    "    normalized_w.append(f'{indicator}_w')\n",
    "    normalized_r.append(f'{indicator}_r')\n",
    "    normalized_i.append(f'{indicator}_i')\n",
    "\n",
    "desired_order = base_columns + year_value_pairs + normalized_w + normalized_r + normalized_i\n",
    "\n",
    "# Only include columns that actually exist in the dataframe\n",
    "existing_columns = [col for col in desired_order if col in final_df.columns]\n",
    "final_df = final_df[existing_columns]\n",
    "\n",
    "# Sort by country code\n",
    "final_df = final_df.sort_values('code').reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_filename = 'ic_frm.csv'\n",
    "final_df.to_csv(output_filename, index=False)\n",
    "print(f\"Data saved to {output_filename}\")\n",
    "print(f\"Final dataset shape: {final_df.shape}\")\n",
    "print(f\"Columns: {list(final_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicator(indicator_code: str, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and clean a single World Bank indicator\n",
    "    \"\"\"\n",
    "    url = \"https://api.worldbank.org/v2/country/all/indicator/\" + indicator_code\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'mrnev': '1',\n",
    "        'per_page': '1000'\n",
    "    }\n",
    "    headers = {\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # The API returns [metadata, data] - we want the data part\n",
    "        if len(data) < 2:\n",
    "            print(f\"Warning: No data returned for indicator {indicator_code}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        records = data[1]\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        # Extract country code and rename columns\n",
    "        df['Country Code'] = df['country'].apply(lambda x: x['id'] if x else None)\n",
    "        df[f'Year_{column_name}'] = df['date']\n",
    "        df[column_name] = pd.to_numeric(df['value'], errors='coerce')\n",
    "        \n",
    "        # Keep only the columns we need\n",
    "        df = df[['Country Code', f'Year_{column_name}', column_name]]\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading indicator {indicator_code}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_country_codes() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load country codes. Since the original references a Country_codes table,\n",
    "    we'll create a basic one or you can replace this with your actual country codes data.\n",
    "    \"\"\"\n",
    "    # This is a placeholder - replace with your actual country codes data\n",
    "    # For now, we'll create it from the World Bank countries API\n",
    "    try:\n",
    "        url = \"https://api.worldbank.org/v2/country\"\n",
    "        params = {'format': 'json', 'per_page': '300'}\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if len(data) < 2:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        countries = data[1]\n",
    "        df = pd.DataFrame(countries)\n",
    "        \n",
    "        # Create the expected columns\n",
    "        df['code_2'] = df['id']\n",
    "        df['code'] = df['id']  # Using same for now\n",
    "        df['region'] = df['region'].apply(lambda x: x['value'] if x and isinstance(x, dict) else 'Unknown')\n",
    "        df['incomegroup'] = df['incomeLevel'].apply(lambda x: x['value'] if x and isinstance(x, dict) else 'Unknown')\n",
    "        \n",
    "        return df[['code', 'code_2', 'region', 'incomegroup']]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading country codes: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def normalize_globally(df: pd.DataFrame, indicators: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply global min-max normalization (suffix _w)\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    for indicator in indicators:\n",
    "        if indicator in result.columns:\n",
    "            values = result[indicator].dropna()\n",
    "            if len(values) > 0:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                \n",
    "                if min_val != max_val:\n",
    "                    result[f'{indicator}_w'] = (result[indicator] - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    result[f'{indicator}_w'] = None\n",
    "            else:\n",
    "                result[f'{indicator}_w'] = None\n",
    "    \n",
    "    return result\n",
    "\n",
    "def normalize_by_group(df: pd.DataFrame, indicators: List[str], group_col: str, suffix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply min-max normalization by group (region or income group)\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    for indicator in indicators:\n",
    "        if indicator in result.columns:\n",
    "            # Calculate min/max by group\n",
    "            group_stats = result.groupby(group_col)[indicator].agg(['min', 'max']).reset_index()\n",
    "            group_stats.columns = [group_col, 'min_val', 'max_val']\n",
    "            \n",
    "            # Merge back with original data\n",
    "            merged = result.merge(group_stats, on=group_col, how='left')\n",
    "            \n",
    "            # Apply normalization\n",
    "            def normalize_row(row):\n",
    "                if pd.isna(row['min_val']) or pd.isna(row['max_val']) or row['min_val'] == row['max_val']:\n",
    "                    return None\n",
    "                elif pd.isna(row[indicator]):\n",
    "                    return None\n",
    "                else:\n",
    "                    return (row[indicator] - row['min_val']) / (row['max_val'] - row['min_val'])\n",
    "            \n",
    "            result[f'{indicator}_{suffix}'] = merged.apply(normalize_row, axis=1)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading World Bank indicators...\n",
      "Loading FDI data...\n",
      "Loading Portfolio Investment data...\n",
      "Loading Private Equity data...\n",
      "Loading country codes...\n",
      "Merging data...\n",
      "Applying normalizations...\n",
      "Data saved to bop.csv\n",
      "Shape: (247, 16)\n",
      "\n",
      "First few rows:\n",
      "  code Year_FDI       FDI  Portfolio Investment (Outflows)  \\\n",
      "0  AFG     2021  0.144467                     6.061967e+07   \n",
      "1  AGO     2023 -2.498840                    -3.848869e+07   \n",
      "2  ALB     2023  6.883977                     3.493903e+08   \n",
      "3  ARG     2023  3.694019                     5.420481e+09   \n",
      "4  ARM     2023  2.409579                    -6.446774e+07   \n",
      "\n",
      "  Year_Portfolio Investment (Outflows)  Private Equity Inflows  \\\n",
      "0                                 2020            0.000000e+00   \n",
      "1                                 2024            0.000000e+00   \n",
      "2                                 2023           -4.191680e+06   \n",
      "3                                 2023           -9.011614e+07   \n",
      "4                                 2023            8.014375e+05   \n",
      "\n",
      "  Year_Private Equity Inflows     FDI_w  Portfolio Investment (Outflows)_w  \\\n",
      "0                        2020  0.767895                           0.837701   \n",
      "1                        2023  0.766338                           0.837629   \n",
      "2                        2023  0.771866                           0.837912   \n",
      "3                        2023  0.769986                           0.841607   \n",
      "4                        2023  0.769230                           0.837610   \n",
      "\n",
      "   Private Equity Inflows_w     FDI_r  Portfolio Investment (Outflows)_r  \\\n",
      "0                  0.098922  0.000000                           0.985531   \n",
      "1                  0.098922  0.000000                           0.485427   \n",
      "2                  0.098910  0.804275                           0.657092   \n",
      "3                  0.098673  0.286696                           0.572568   \n",
      "4                  0.098924  0.316714                           0.634486   \n",
      "\n",
      "   Private Equity Inflows_r     FDI_i  Portfolio Investment (Outflows)_i  \\\n",
      "0                  0.006216  0.095525                           0.576791   \n",
      "1                  0.592994  0.000000                           0.822973   \n",
      "2                  0.635258  0.435547                           0.160554   \n",
      "3                  0.850786  0.281551                           0.228237   \n",
      "4                  0.636567  0.219544                           0.155031   \n",
      "\n",
      "   Private Equity Inflows_i  \n",
      "0                  0.834781  \n",
      "1                  0.011665  \n",
      "2                  0.431433  \n",
      "3                  0.424876  \n",
      "4                  0.431814  \n"
     ]
    }
   ],
   "source": [
    "print(\"Loading World Bank indicators...\")\n",
    "    \n",
    "# Load each indicator\n",
    "print(\"Loading FDI data...\")\n",
    "fdi = load_indicator(\"BX.KLT.DINV.WD.GD.ZS\", \"FDI\")\n",
    "\n",
    "print(\"Loading Portfolio Investment data...\")\n",
    "portfolio = load_indicator(\"BN.KLT.PTXL.CD\", \"Portfolio Investment (Outflows)\")\n",
    "\n",
    "print(\"Loading Private Equity data...\")\n",
    "private_equity = load_indicator(\"BX.PEF.TOTL.CD.WD\", \"Private Equity Inflows\")\n",
    "\n",
    "print(\"Loading country codes...\")\n",
    "country_codes = country_code_df\n",
    "\n",
    "# Merge indicators step by step\n",
    "print(\"Merging data...\")\n",
    "\n",
    "# Start with FDI as base\n",
    "result = fdi.copy()\n",
    "\n",
    "# Merge Portfolio Investment\n",
    "if not portfolio.empty:\n",
    "    result = result.merge(\n",
    "        portfolio[['Country Code', 'Portfolio Investment (Outflows)', 'Year_Portfolio Investment (Outflows)']], \n",
    "        on='Country Code', \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "# Merge Private Equity\n",
    "if not private_equity.empty:\n",
    "    result = result.merge(\n",
    "        private_equity[['Country Code', 'Private Equity Inflows', 'Year_Private Equity Inflows']], \n",
    "        on='Country Code', \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "# Merge with country codes\n",
    "result = country_codes.merge(result, left_on='code_2', right_on='Country Code', how='right')\n",
    "\n",
    "# Define indicators for normalization\n",
    "indicators = [\"FDI\", \"Portfolio Investment (Outflows)\", \"Private Equity Inflows\"]\n",
    "\n",
    "print(\"Applying normalizations...\")\n",
    "\n",
    "# Apply global normalization\n",
    "result = normalize_globally(result, indicators)\n",
    "\n",
    "# Apply regional normalization\n",
    "result = normalize_by_group(result, indicators, 'region', 'r')\n",
    "\n",
    "# Apply income group normalization\n",
    "result = normalize_by_group(result, indicators, 'incomegroup', 'i')\n",
    "\n",
    "# Clean up columns\n",
    "columns_to_keep = [\n",
    "    'code', 'Year_FDI', 'FDI', \n",
    "    'Portfolio Investment (Outflows)', 'Year_Portfolio Investment (Outflows)',\n",
    "    'Private Equity Inflows', 'Year_Private Equity Inflows',\n",
    "    'FDI_w', 'Portfolio Investment (Outflows)_w', 'Private Equity Inflows_w',\n",
    "    'FDI_r', 'Portfolio Investment (Outflows)_r', 'Private Equity Inflows_r',\n",
    "    'FDI_i', 'Portfolio Investment (Outflows)_i', 'Private Equity Inflows_i'\n",
    "]\n",
    "\n",
    "# Keep only columns that exist in the DataFrame\n",
    "existing_columns = [col for col in columns_to_keep if col in result.columns]\n",
    "result = result[existing_columns]\n",
    "\n",
    "# Sort by country code\n",
    "result = result.sort_values('code').reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'bop.csv'\n",
    "result.to_csv(output_file, index=False)\n",
    "print(f\"Data saved to {output_file}\")\n",
    "print(f\"Shape: {result.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bop_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicator(indicator_code: str, column_name: str, page: str = \"1\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and clean a single World Bank indicator, calculating change over time\n",
    "    \"\"\"\n",
    "    url = \"https://api.worldbank.org/v2/country/all/indicator/\" + indicator_code\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'mrnev': '5',  # Most recent non-empty values\n",
    "        'per_page': '1000',\n",
    "        'page': page\n",
    "    }\n",
    "    headers = {\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # The API returns [metadata, data] - we want the data part\n",
    "        if len(data) < 2:\n",
    "            print(f\"Warning: No data returned for indicator {indicator_code}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        records = data[1]\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        # Extract relevant fields\n",
    "        df['Country Code'] = df['country'].apply(lambda x: x['id'] if x else None)\n",
    "        df['Year'] = pd.to_numeric(df['date'], errors='coerce')\n",
    "        df[column_name] = pd.to_numeric(df['value'], errors='coerce')\n",
    "        \n",
    "        # Remove rows with null essential values\n",
    "        df = df.dropna(subset=['Country Code', 'Year', column_name])\n",
    "        \n",
    "        # Sort by Country Code and Year\n",
    "        df = df.sort_values(['Country Code', 'Year'])\n",
    "        \n",
    "        # Group by country and calculate change over time\n",
    "        def calculate_change(group):\n",
    "            if len(group) < 2:\n",
    "                return pd.Series({\n",
    "                    'Country Code': group['Country Code'].iloc[0],\n",
    "                    f'd_{column_name}': None,\n",
    "                    f'Year Range_{column_name}': None\n",
    "                })\n",
    "            \n",
    "            first_year = group['Year'].min()\n",
    "            last_year = group['Year'].max()\n",
    "            start_val = group[group['Year'] == first_year][column_name].iloc[0]\n",
    "            end_val = group[group['Year'] == last_year][column_name].iloc[-1]\n",
    "            \n",
    "            # Calculate rate of change per year\n",
    "            year_diff = last_year - first_year\n",
    "            if year_diff > 0:\n",
    "                change_rate = (end_val - start_val) / year_diff\n",
    "            else:\n",
    "                change_rate = None\n",
    "            \n",
    "            year_range = f\"{int(first_year)}–{int(last_year)}\"\n",
    "            \n",
    "            return pd.Series({\n",
    "                'Country Code': group['Country Code'].iloc[0],\n",
    "                f'd_{column_name}': change_rate,\n",
    "                f'Year Range_{column_name}': year_range\n",
    "            })\n",
    "        \n",
    "        # Apply the calculation\n",
    "        result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading indicator {indicator_code}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_country_codes() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load country codes with region and income group information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://api.worldbank.org/v2/country\"\n",
    "        params = {'format': 'json', 'per_page': '300'}\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if len(data) < 2:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        countries = data[1]\n",
    "        df = pd.DataFrame(countries)\n",
    "        \n",
    "        # Create the expected columns\n",
    "        df['code_2'] = df['id']\n",
    "        df['code'] = df['id']\n",
    "        df['region'] = df['region'].apply(lambda x: x['value'] if x and isinstance(x, dict) else 'Unknown')\n",
    "        df['incomegroup'] = df['incomeLevel'].apply(lambda x: x['value'] if x and isinstance(x, dict) else 'Unknown')\n",
    "        \n",
    "        return df[['code', 'code_2', 'region', 'incomegroup']]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading country codes: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def normalize_globally(df: pd.DataFrame, indicators: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply global min-max normalization (suffix _w)\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    for indicator in indicators:\n",
    "        if indicator in result.columns:\n",
    "            values = result[indicator].dropna()\n",
    "            if len(values) > 0:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                \n",
    "                if min_val != max_val and pd.notna(min_val) and pd.notna(max_val):\n",
    "                    result[f'{indicator}_w'] = (result[indicator] - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    result[f'{indicator}_w'] = None\n",
    "            else:\n",
    "                result[f'{indicator}_w'] = None\n",
    "    \n",
    "    return result\n",
    "\n",
    "def normalize_by_group(df: pd.DataFrame, indicators: List[str], group_col: str, suffix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply min-max normalization by group (region or income group)\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    for indicator in indicators:\n",
    "        if indicator in result.columns:\n",
    "            # Calculate min/max by group\n",
    "            group_stats = result.groupby(group_col)[indicator].agg(['min', 'max']).reset_index()\n",
    "            group_stats.columns = [group_col, 'min_val', 'max_val']\n",
    "            \n",
    "            # Merge back with original data\n",
    "            merged = result.merge(group_stats, on=group_col, how='left')\n",
    "            \n",
    "            # Apply normalization\n",
    "            def normalize_row(row):\n",
    "                if (pd.isna(row['min_val']) or pd.isna(row['max_val']) or \n",
    "                    row['min_val'] == row['max_val'] or pd.isna(row[indicator])):\n",
    "                    return None\n",
    "                else:\n",
    "                    return (row[indicator] - row['min_val']) / (row['max_val'] - row['min_val'])\n",
    "            \n",
    "            result[f'{indicator}_{suffix}'] = merged.apply(normalize_row, axis=1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading World Bank indicators for change analysis...\n",
      "Loading FDI change data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3047608852.py:72: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Portfolio Investment change data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3047608852.py:72: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Private Equity change data...\n",
      "Loading country codes...\n",
      "Merging data...\n",
      "Applying normalizations...\n",
      "Data saved to bop_d.csv\n",
      "Shape: (201, 16)\n",
      "\n",
      "First few rows:\n",
      "  code     d_FDI Year Range_FDI  d_Portfolio Investment (Outflows)  \\\n",
      "0  AFG -0.032582      2017–2021                      -9.712522e+06   \n",
      "1  AGO  0.820493      2019–2023                       4.004684e+08   \n",
      "2  ALB -0.205560      2019–2023                       4.867152e+07   \n",
      "3  ARG  0.552253      2019–2023                      -4.220251e+08   \n",
      "4  ARM  0.418304      2019–2023                      -8.901979e+06   \n",
      "\n",
      "  Year Range_Portfolio Investment (Outflows)  d_Private Equity Inflows  \\\n",
      "0                                  2016–2020              0.000000e+00   \n",
      "1                                  2020–2024              0.000000e+00   \n",
      "2                                  2019–2023             -2.795264e+06   \n",
      "3                                  2019–2023             -4.392888e+07   \n",
      "4                                  2019–2023              1.351188e+06   \n",
      "\n",
      "  Year Range_Private Equity Inflows   d_FDI_w   d_FDI_r   d_FDI_i  \\\n",
      "0                         2016–2020  0.793318  0.940466  0.369759   \n",
      "1                         2019–2023  0.795193  0.423313  0.372487   \n",
      "2                         2019–2023  0.792937  0.248624  0.201696   \n",
      "3                         2019–2023  0.794603  0.885089  0.318003   \n",
      "4                         2019–2023  0.794309  0.566767  0.297445   \n",
      "\n",
      "   d_Portfolio Investment (Outflows)_w  d_Portfolio Investment (Outflows)_r  \\\n",
      "0                             0.419638                             1.000000   \n",
      "1                             0.427493                             0.636066   \n",
      "2                             0.420756                             0.050675   \n",
      "3                             0.411743                             0.393644   \n",
      "4                             0.419654                             0.038129   \n",
      "\n",
      "   d_Portfolio Investment (Outflows)_i  d_Private Equity Inflows_w  \\\n",
      "0                             0.408726                    0.508237   \n",
      "1                             0.451759                    0.508237   \n",
      "2                             0.109561                    0.508223   \n",
      "3                             0.095705                    0.508015   \n",
      "4                             0.107866                    0.508243   \n",
      "\n",
      "   d_Private Equity Inflows_r  d_Private Equity Inflows_i  \n",
      "0                    0.008325                    0.461886  \n",
      "1                    0.048725                    0.151516  \n",
      "2                    0.076196                    0.934602  \n",
      "3                    0.641189                    0.930496  \n",
      "4                    0.084243                    0.935016  \n",
      "\n",
      "Column names:\n",
      "['code', 'd_FDI', 'Year Range_FDI', 'd_Portfolio Investment (Outflows)', 'Year Range_Portfolio Investment (Outflows)', 'd_Private Equity Inflows', 'Year Range_Private Equity Inflows', 'd_FDI_w', 'd_FDI_r', 'd_FDI_i', 'd_Portfolio Investment (Outflows)_w', 'd_Portfolio Investment (Outflows)_r', 'd_Portfolio Investment (Outflows)_i', 'd_Private Equity Inflows_w', 'd_Private Equity Inflows_r', 'd_Private Equity Inflows_i']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3047608852.py:72: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('Country Code').apply(calculate_change).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading World Bank indicators for change analysis...\")\n",
    "\n",
    "# Load each indicator with change calculations\n",
    "print(\"Loading FDI change data...\")\n",
    "fdi = load_indicator(\"BX.KLT.DINV.WD.GD.ZS\", \"FDI\", \"1\")\n",
    "\n",
    "print(\"Loading Portfolio Investment change data...\")\n",
    "portfolio = load_indicator(\"BN.KLT.PTXL.CD\", \"Portfolio Investment (Outflows)\", \"1\")\n",
    "\n",
    "print(\"Loading Private Equity change data...\")\n",
    "private_equity = load_indicator(\"BX.PEF.TOTL.CD.WD\", \"Private Equity Inflows\", \"1\")\n",
    "\n",
    "print(\"Loading country codes...\")\n",
    "country_codes = country_code_df\n",
    "\n",
    "if fdi.empty or country_codes.empty:\n",
    "    print(\"Error: Could not load required data\")\n",
    "    \n",
    "\n",
    "# Merge indicators step by step\n",
    "print(\"Merging data...\")\n",
    "\n",
    "# Start with FDI as base\n",
    "result = fdi.copy()\n",
    "\n",
    "# Merge Portfolio Investment\n",
    "if not portfolio.empty:\n",
    "    portfolio_cols = ['Country Code', 'd_Portfolio Investment (Outflows)', 'Year Range_Portfolio Investment (Outflows)']\n",
    "    existing_portfolio_cols = [col for col in portfolio_cols if col in portfolio.columns]\n",
    "    result = result.merge(portfolio[existing_portfolio_cols], on='Country Code', how='left')\n",
    "\n",
    "# Merge Private Equity\n",
    "if not private_equity.empty:\n",
    "    equity_cols = ['Country Code', 'd_Private Equity Inflows', 'Year Range_Private Equity Inflows']\n",
    "    existing_equity_cols = [col for col in equity_cols if col in private_equity.columns]\n",
    "    result = result.merge(private_equity[existing_equity_cols], on='Country Code', how='left')\n",
    "\n",
    "# Merge with country codes (right outer join to keep all countries)\n",
    "result = country_codes.merge(result, left_on='code_2', right_on='Country Code', how='right')\n",
    "\n",
    "# Define indicators for normalization (change rates)\n",
    "indicators = [\"d_FDI\", \"d_Portfolio Investment (Outflows)\", \"d_Private Equity Inflows\"]\n",
    "\n",
    "# Filter indicators that actually exist in the data\n",
    "existing_indicators = [ind for ind in indicators if ind in result.columns]\n",
    "\n",
    "if existing_indicators:\n",
    "    print(\"Applying normalizations...\")\n",
    "    \n",
    "    # Apply global normalization\n",
    "    result = normalize_globally(result, existing_indicators)\n",
    "    \n",
    "    # Apply regional normalization\n",
    "    result = normalize_by_group(result, existing_indicators, 'region', 'r')\n",
    "    \n",
    "    # Apply income group normalization\n",
    "    result = normalize_by_group(result, existing_indicators, 'incomegroup', 'i')\n",
    "\n",
    "# Define desired column order\n",
    "base_columns = ['code']\n",
    "\n",
    "# Add change rate columns and year ranges\n",
    "for indicator_name in ['FDI', 'Portfolio Investment (Outflows)', 'Private Equity Inflows']:\n",
    "    d_col = f'd_{indicator_name}'\n",
    "    year_col = f'Year Range_{indicator_name}'\n",
    "    if d_col in result.columns:\n",
    "        base_columns.append(d_col)\n",
    "    if year_col in result.columns:\n",
    "        base_columns.append(year_col)\n",
    "\n",
    "# Add normalized columns\n",
    "for indicator in existing_indicators:\n",
    "    for suffix in ['_w', '_r', '_i']:\n",
    "        norm_col = f'{indicator}{suffix}'\n",
    "        if norm_col in result.columns:\n",
    "            base_columns.append(norm_col)\n",
    "\n",
    "# Keep only columns that exist in the DataFrame\n",
    "existing_columns = [col for col in base_columns if col in result.columns]\n",
    "result = result[existing_columns]\n",
    "\n",
    "# Sort by country code\n",
    "result = result.sort_values('code').reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'bop_d.csv'\n",
    "result.to_csv(output_file, index=False)\n",
    "print(f\"Data saved to {output_file}\")\n",
    "print(f\"Shape: {result.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(result.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicator(indicator_code, column_name):\n",
    "    \"\"\"\n",
    "    Function to load and clean a single indicator from World Bank API\n",
    "    \"\"\"\n",
    "    url = \"https://api.worldbank.org/v2/country/all/indicator/\" + indicator_code\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'mrnev': '1',\n",
    "        'per_page': '1000'\n",
    "    }\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # The API returns [metadata, data] - we want the data part\n",
    "        if len(data) > 1:\n",
    "            records = data[1]\n",
    "        else:\n",
    "            records = []\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Extract country code and rename columns\n",
    "            df['Country Code'] = df['country'].apply(lambda x: x['id'] if x else None)\n",
    "            df = df.rename(columns={\n",
    "                'date': f'Year_{column_name}',\n",
    "                'value': column_name\n",
    "            })\n",
    "            \n",
    "            # Keep only relevant columns\n",
    "            df = df[['Country Code', f'Year_{column_name}', column_name]]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading indicator {indicator_code}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_country_codes():\n",
    "    \"\"\"\n",
    "    Load country codes with region and income group information\n",
    "    \"\"\"\n",
    "    url = \"https://api.worldbank.org/v2/country\"\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'per_page': '500'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if len(data) > 1:\n",
    "            countries = data[1]\n",
    "        else:\n",
    "            countries = []\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(countries)\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Extract relevant information\n",
    "            df['code'] = df['id']\n",
    "            df['code_2'] = df['iso2Code']\n",
    "            df['region'] = df['region'].apply(lambda x: x['value'] if x and x['value'] != 'Aggregates' else None)\n",
    "            df['incomegroup'] = df['incomeLevel'].apply(lambda x: x['value'] if x else None)\n",
    "            \n",
    "            # Keep only countries (not aggregates)\n",
    "            df = df[df['region'].notna()]\n",
    "            df = df[['code', 'code_2', 'region', 'incomegroup']]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading country codes: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def normalize_columns(df, indicators, suffix, group_by=None):\n",
    "    \"\"\"\n",
    "    Perform min-max normalization on specified columns\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for indicator in indicators:\n",
    "        new_col_name = f\"{indicator}_{suffix}\"\n",
    "        \n",
    "        if group_by:\n",
    "            # Group-wise normalization\n",
    "            def normalize_group(group):\n",
    "                values = group[indicator].dropna()\n",
    "                if len(values) == 0 or values.min() == values.max():\n",
    "                    return pd.Series([None] * len(group), index=group.index)\n",
    "                else:\n",
    "                    min_val = values.min()\n",
    "                    max_val = values.max()\n",
    "                    return (group[indicator] - min_val) / (max_val - min_val)\n",
    "            \n",
    "            df_copy[new_col_name] = df_copy.groupby(group_by, group_keys=False).apply(normalize_group).values\n",
    "        else:\n",
    "            # Global normalization\n",
    "            values = df_copy[indicator].dropna()\n",
    "            if len(values) == 0 or values.min() == values.max():\n",
    "                df_copy[new_col_name] = None\n",
    "            else:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                df_copy[new_col_name] = (df_copy[indicator] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading country codes...\n",
      "Loading Economic Management indicator...\n",
      "Merging with country codes...\n",
      "Performing normalizations...\n",
      "Data saved to CPIA.csv\n",
      "Shape: (135, 6)\n",
      "\n",
      "First few rows:\n",
      "  code Year_EconMgmt  EconMgmt  EconMgmt_w  EconMgmt_r  EconMgmt_i\n",
      "0  AFG          2023  1.166667    0.000000    0.928572    0.000000\n",
      "3  AGO          2013  3.000000    0.523809    1.000000    0.705882\n",
      "1  ALB          2006  4.000000    0.809524         NaN    0.823529\n",
      "4  ARG           NaN       NaN         NaN    0.285714    0.705882\n",
      "5  ARM          2013  4.500000    0.952381         NaN    0.941176\n",
      "\n",
      "Number of countries: 135\n",
      "Number of records with EconMgmt data: 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\2305979820.py:104: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_copy[new_col_name] = df_copy.groupby(group_by, group_keys=False).apply(normalize_group).values\n",
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\2305979820.py:104: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_copy[new_col_name] = df_copy.groupby(group_by, group_keys=False).apply(normalize_group).values\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading country codes...\")\n",
    "country_codes = country_code_df\n",
    "\n",
    "if country_codes.empty:\n",
    "    print(\"Failed to load country codes\")\n",
    "    \n",
    "\n",
    "print(\"Loading Economic Management indicator...\")\n",
    "# Load Economic Management indicator (IQ.CPA.ECON.XQ)\n",
    "econ_mgmt = load_indicator(\"IQ.CPA.ECON.XQ\", \"EconMgmt\")\n",
    "\n",
    "if econ_mgmt.empty:\n",
    "    print(\"Failed to load Economic Management data\")\n",
    "    \n",
    "\n",
    "print(\"Merging with country codes...\")\n",
    "# Merge with country codes (right outer join)\n",
    "result = pd.merge(\n",
    "    country_codes, \n",
    "    econ_mgmt, \n",
    "    left_on='code_2', \n",
    "    right_on='Country Code', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean up the data\n",
    "result = result.drop(columns=['Country Code'])\n",
    "\n",
    "# Convert EconMgmt to numeric\n",
    "result['EconMgmt'] = pd.to_numeric(result['EconMgmt'], errors='coerce')\n",
    "\n",
    "print(\"Performing normalizations...\")\n",
    "# List of indicators to normalize\n",
    "indicators = ['EconMgmt']\n",
    "\n",
    "# 1. Global min-max normalization (suffix _w)\n",
    "result = normalize_columns(result, indicators, 'w')\n",
    "\n",
    "# 2. Regional min-max normalization (suffix _r)\n",
    "result = normalize_columns(result, indicators, 'r', group_by='region')\n",
    "\n",
    "# 3. Income group min-max normalization (suffix _i)\n",
    "result = normalize_columns(result, indicators, 'i', group_by='incomegroup')\n",
    "\n",
    "# Remove unnecessary columns and reorder\n",
    "final_columns = ['code', 'Year_EconMgmt', 'EconMgmt', 'EconMgmt_w', 'EconMgmt_r', 'EconMgmt_i']\n",
    "available_columns = [col for col in final_columns if col in result.columns]\n",
    "result = result[available_columns]\n",
    "\n",
    "# Sort by country code\n",
    "result = result.sort_values('code')\n",
    "\n",
    "# Save to CSV\n",
    "result.to_csv('CPIA.csv', index=False)\n",
    "print(\"Data saved to CPIA.csv\")\n",
    "print(f\"Shape: {result.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result.head())\n",
    "\n",
    "# Display some statistics\n",
    "print(f\"\\nNumber of countries: {result['code'].nunique()}\")\n",
    "print(f\"Number of records with EconMgmt data: {result['EconMgmt'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPIA_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicator_delta(indicator_code, column_name, page=\"1\"):\n",
    "    \"\"\"\n",
    "    Function to load indicator data and calculate change over time\n",
    "    \"\"\"\n",
    "    url = \"https://api.worldbank.org/v2/country/all/indicator/\" + indicator_code\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'mrnev': '5',  # Most recent 5 values\n",
    "        'per_page': '1000',\n",
    "        'page': page\n",
    "    }\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    \n",
    "    try:\n",
    "        print(f\"Making API request to: {url}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        \n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        print(f\"Response status code: {response.status_code}\")\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        print(f\"API response structure: {type(data)}, length: {len(data) if isinstance(data, list) else 'N/A'}\")\n",
    "        \n",
    "        # The API returns [metadata, data] - we want the data part\n",
    "        if len(data) > 1 and data[1] is not None:\n",
    "            records = data[1]\n",
    "            print(f\"Found {len(records)} records\")\n",
    "        else:\n",
    "            print(\"No data found in API response\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        if not records:\n",
    "            print(\"Records list is empty\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(records)\n",
    "        print(f\"Created DataFrame with shape: {df.shape}\")\n",
    "        print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Check if required fields exist\n",
    "        if 'country' not in df.columns or 'date' not in df.columns or 'value' not in df.columns:\n",
    "            print(f\"Missing required columns. Available: {df.columns.tolist()}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Extract country code and clean data\n",
    "        df['Country Code'] = df['country'].apply(lambda x: x['id'] if x and isinstance(x, dict) else None)\n",
    "        df['Year'] = pd.to_numeric(df['date'], errors='coerce').astype('Int64')\n",
    "        df[column_name] = pd.to_numeric(df['value'], errors='coerce')\n",
    "        \n",
    "        print(f\"After cleaning, DataFrame shape: {df.shape}\")\n",
    "        print(f\"Non-null values - Country Code: {df['Country Code'].notna().sum()}, Year: {df['Year'].notna().sum()}, {column_name}: {df[column_name].notna().sum()}\")\n",
    "        \n",
    "        # Keep only relevant columns and remove nulls\n",
    "        df = df[['Country Code', 'Year', column_name]].dropna()\n",
    "        print(f\"After removing nulls, DataFrame shape: {df.shape}\")\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"DataFrame is empty after removing nulls\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Sort by country and year\n",
    "        df = df.sort_values(['Country Code', 'Year'])\n",
    "        \n",
    "        # Group by country and calculate change over time\n",
    "        def calculate_delta(group):\n",
    "            if len(group) < 2:\n",
    "                return pd.Series({\n",
    "                    'Country Code': group['Country Code'].iloc[0],\n",
    "                    'FirstYear': None,\n",
    "                    'LastYear': None,\n",
    "                    'StartVal': None,\n",
    "                    'EndVal': None,\n",
    "                    f'd_{column_name}': None,\n",
    "                    f'Year Range_{column_name}': None\n",
    "                })\n",
    "            \n",
    "            first_year = group['Year'].min()\n",
    "            last_year = group['Year'].max()\n",
    "            start_val = group[group['Year'] == first_year][column_name].iloc[0]\n",
    "            end_val = group[group['Year'] == last_year][column_name].iloc[-1]\n",
    "            \n",
    "            # Calculate change per year\n",
    "            year_diff = last_year - first_year\n",
    "            if year_diff == 0:\n",
    "                delta = 0\n",
    "            else:\n",
    "                delta = (end_val - start_val) / year_diff\n",
    "            \n",
    "            year_range = f\"{first_year}–{last_year}\"\n",
    "            \n",
    "            return pd.Series({\n",
    "                'Country Code': group['Country Code'].iloc[0],\n",
    "                'FirstYear': first_year,\n",
    "                'LastYear': last_year,\n",
    "                'StartVal': start_val,\n",
    "                'EndVal': end_val,\n",
    "                f'd_{column_name}': delta,\n",
    "                f'Year Range_{column_name}': year_range\n",
    "            })\n",
    "        \n",
    "        # Apply the calculation to each country\n",
    "        grouped_data = df.groupby('Country Code')\n",
    "        print(f\"Number of countries with data: {len(grouped_data)}\")\n",
    "        \n",
    "        result = grouped_data.apply(calculate_delta, include_groups=False).reset_index(drop=True)\n",
    "        print(f\"Result after delta calculation shape: {result.shape}\")\n",
    "        \n",
    "        # Keep only the final columns we need\n",
    "        final_columns = ['Country Code', f'd_{column_name}', f'Year Range_{column_name}']\n",
    "        available_final_columns = [col for col in final_columns if col in result.columns]\n",
    "        \n",
    "        if available_final_columns:\n",
    "            result = result[available_final_columns]\n",
    "        else:\n",
    "            print(f\"None of the expected final columns found. Available: {result.columns.tolist()}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Final result shape: {result.shape}\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading indicator {indicator_code}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_country_codes():\n",
    "    \"\"\"\n",
    "    Load country codes with region and income group information\n",
    "    \"\"\"\n",
    "    url = \"https://api.worldbank.org/v2/country\"\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'per_page': '500'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading country codes from: {url}\")\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if len(data) > 1:\n",
    "            countries = data[1]\n",
    "        else:\n",
    "            countries = []\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(countries)\n",
    "        print(f\"Country codes raw DataFrame shape: {df.shape}\")\n",
    "        print(f\"Country codes raw columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Extract relevant information - handle the structure we're seeing\n",
    "            df['code'] = df['id']\n",
    "            df['code_2'] = df['iso2Code'] \n",
    "            \n",
    "            # Handle region - it might be a string or dict\n",
    "            if 'region' in df.columns:\n",
    "                df['region'] = df['region'].apply(\n",
    "                    lambda x: x['value'] if isinstance(x, dict) and x and x.get('value') != 'Aggregates' \n",
    "                    else x if isinstance(x, str) and x != 'Aggregates' \n",
    "                    else None\n",
    "                )\n",
    "            \n",
    "            # Handle income group - it might be a string or dict  \n",
    "            if 'incomeLevel' in df.columns:\n",
    "                df['incomegroup'] = df['incomeLevel'].apply(\n",
    "                    lambda x: x['value'] if isinstance(x, dict) and x \n",
    "                    else x if isinstance(x, str) \n",
    "                    else None\n",
    "                )\n",
    "            elif 'incomegroup' not in df.columns:\n",
    "                df['incomegroup'] = None\n",
    "            \n",
    "            # Keep only countries (not aggregates)\n",
    "            if 'region' in df.columns:\n",
    "                df = df[df['region'].notna()]\n",
    "            \n",
    "            # Keep relevant columns\n",
    "            keep_cols = ['code', 'code_2', 'region', 'incomegroup']\n",
    "            available_cols = [col for col in keep_cols if col in df.columns]\n",
    "            df = df[available_cols]\n",
    "            \n",
    "            print(f\"Final country codes DataFrame shape: {df.shape}\")\n",
    "            print(f\"Final country codes columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading country codes: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def normalize_columns(df, indicators, suffix, group_by=None):\n",
    "    \"\"\"\n",
    "    Perform min-max normalization on specified columns\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for indicator in indicators:\n",
    "        new_col_name = f\"{indicator}_{suffix}\"\n",
    "        \n",
    "        if group_by:\n",
    "            # Group-wise normalization\n",
    "            def normalize_group(group):\n",
    "                values = group[indicator].dropna()\n",
    "                if len(values) == 0 or values.min() == values.max():\n",
    "                    return pd.Series([None] * len(group), index=group.index)\n",
    "                else:\n",
    "                    min_val = values.min()\n",
    "                    max_val = values.max()\n",
    "                    return (group[indicator] - min_val) / (max_val - min_val)\n",
    "            \n",
    "            df_copy[new_col_name] = df_copy.groupby(group_by, group_keys=False).apply(normalize_group).values\n",
    "        else:\n",
    "            # Global normalization\n",
    "            values = df_copy[indicator].dropna()\n",
    "            if len(values) == 0 or values.min() == values.max():\n",
    "                df_copy[new_col_name] = None\n",
    "            else:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                df_copy[new_col_name] = (df_copy[indicator] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading country codes...\n",
      "Loading Economic Management indicator and calculating deltas...\n",
      "Making API request to: https://api.worldbank.org/v2/country/all/indicator/IQ.CPA.ECON.XQ\n",
      "Parameters: {'format': 'json', 'mrnev': '5', 'per_page': '1000', 'page': '1'}\n",
      "Response status code: 200\n",
      "API response structure: <class 'list'>, length: 2\n",
      "Found 640 records\n",
      "Created DataFrame with shape: (640, 7)\n",
      "DataFrame columns: ['indicator', 'country', 'countryiso3code', 'date', 'value', 'obs_status', 'decimal']\n",
      "After cleaning, DataFrame shape: (640, 10)\n",
      "Non-null values - Country Code: 640, Year: 640, EconMgmt: 640\n",
      "After removing nulls, DataFrame shape: (640, 3)\n",
      "Number of countries with data: 130\n",
      "Error loading indicator IQ.CPA.ECON.XQ: 'Country Code'\n",
      "Failed to load Economic Management delta data\n",
      "Delta data columns: []\n",
      "Delta data shape: (0, 0)\n",
      "Country codes columns: ['code', 'country', 'region', 'incomegroup', 'ifs', 'imf_region', 'imf_income', 'code_2', 'country_unctad', 'country_vcpe']\n",
      "Merging with country codes...\n",
      "Warning: 'Country Code' column not found in delta data\n",
      "Available columns in delta data: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\esn\\tutorial\\financial-additionality\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3805, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas\\\\_libs\\\\hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas\\\\_libs\\\\hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'Country Code'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3929222697.py\", line 108, in load_indicator_delta\n",
      "    result = grouped_data.apply(calculate_delta, include_groups=False).reset_index(drop=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\esn\\tutorial\\financial-additionality\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 1819, in apply\n",
      "    return self._python_apply_general(f, self._obj_with_exclusions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\esn\\tutorial\\financial-additionality\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 1885, in _python_apply_general\n",
      "    values, mutated = self._grouper.apply_groupwise(f, data, self.axis)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\esn\\tutorial\\financial-additionality\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py\", line 919, in apply_groupwise\n",
      "    res = f(group)\n",
      "          ^^^^^^^^\n",
      "  File \"C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\3929222697.py\", line 95, in calculate_delta\n",
      "    'Country Code': group['Country Code'].iloc[0],\n",
      "                    ~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\esn\\tutorial\\financial-additionality\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 4102, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\esn\\tutorial\\financial-additionality\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'Country Code'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Country Code'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_8988\\4218306251.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     33\u001b[39m     print(\u001b[33mf\"Available columns in country codes: {country_codes.columns.tolist()}\"\u001b[39m)\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Merge with country codes (right outer join)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m result = pd.merge(\n\u001b[32m     38\u001b[39m     country_codes,\n\u001b[32m     39\u001b[39m     econ_mgmt_delta,\n\u001b[32m     40\u001b[39m     left_on=\u001b[33m'code_2'\u001b[39m,\n",
      "\u001b[32mc:\\Users\\esn\\tutorial\\financial-additionality\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\esn\\tutorial\\financial-additionality\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\esn\\tutorial\\financial-additionality\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1293\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1296\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1298\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1299\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1300\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32mc:\\Users\\esn\\tutorial\\financial-additionality\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'Country Code'"
     ]
    }
   ],
   "source": [
    "print(\"Loading country codes...\")\n",
    "country_codes = country_code_df\n",
    "\n",
    "if country_codes.empty:\n",
    "    print(\"Failed to load country codes\")\n",
    "    \n",
    "\n",
    "if country_codes.empty:\n",
    "    print(\"Failed to load country codes\")\n",
    "    \n",
    "\n",
    "print(\"Loading Economic Management indicator and calculating deltas...\")\n",
    "# Load Economic Management indicator with delta calculation\n",
    "econ_mgmt_delta = load_indicator_delta(\"IQ.CPA.ECON.XQ\", \"EconMgmt\", \"1\")\n",
    "\n",
    "if econ_mgmt_delta.empty:\n",
    "    print(\"Failed to load Economic Management delta data\")\n",
    "    \n",
    "\n",
    "print(f\"Delta data columns: {econ_mgmt_delta.columns.tolist()}\")\n",
    "print(f\"Delta data shape: {econ_mgmt_delta.shape}\")\n",
    "print(f\"Country codes columns: {country_codes.columns.tolist()}\")\n",
    "\n",
    "print(\"Merging with country codes...\")\n",
    "# Check if required columns exist\n",
    "if 'Country Code' not in econ_mgmt_delta.columns:\n",
    "    print(\"Warning: 'Country Code' column not found in delta data\")\n",
    "    print(f\"Available columns in delta data: {econ_mgmt_delta.columns.tolist()}\")\n",
    "    \n",
    "\n",
    "if 'code_2' not in country_codes.columns:\n",
    "    print(\"Warning: 'code_2' column not found in country codes\")\n",
    "    print(f\"Available columns in country codes: {country_codes.columns.tolist()}\")\n",
    "    \n",
    "\n",
    "# Merge with country codes (right outer join)\n",
    "result = pd.merge(\n",
    "    country_codes, \n",
    "    econ_mgmt_delta, \n",
    "    left_on='code_2', \n",
    "    right_on='Country Code', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean up the data - only drop if the column exists\n",
    "if 'Country Code' in result.columns:\n",
    "    result = result.drop(columns=['Country Code'])\n",
    "\n",
    "print(\"Performing normalizations...\")\n",
    "# List of delta indicators to normalize\n",
    "indicators = ['d_EconMgmt']\n",
    "\n",
    "# 1. Global min-max normalization (suffix _w)\n",
    "result = normalize_columns(result, indicators, 'w')\n",
    "\n",
    "# 2. Regional min-max normalization (suffix _r)\n",
    "result = normalize_columns(result, indicators, 'r', group_by='region')\n",
    "\n",
    "# 3. Income group min-max normalization (suffix _i)\n",
    "result = normalize_columns(result, indicators, 'i', group_by='incomegroup')\n",
    "\n",
    "# Remove unnecessary columns and reorder\n",
    "final_columns = ['code', 'd_EconMgmt', 'Year Range_EconMgmt', 'd_EconMgmt_w', 'd_EconMgmt_r', 'd_EconMgmt_i']\n",
    "available_columns = [col for col in final_columns if col in result.columns]\n",
    "result = result[available_columns]\n",
    "\n",
    "# Sort by country code\n",
    "result = result.sort_values('code')\n",
    "\n",
    "# Save to CSV\n",
    "result.to_csv('CPIA_d.csv', index=False)\n",
    "print(\"Data saved to CPIA_d.csv\")\n",
    "print(f\"Shape: {result.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result.head())\n",
    "\n",
    "# Display some statistics\n",
    "print(f\"\\nNumber of countries: {result['code'].nunique()}\")\n",
    "print(f\"Number of records with d_EconMgmt data: {result['d_EconMgmt'].notna().sum()}\")\n",
    "\n",
    "# Show some delta statistics\n",
    "delta_stats = result['d_EconMgmt'].describe()\n",
    "print(f\"\\nDelta EconMgmt statistics:\")\n",
    "print(delta_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicator(indicator_code: str, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to load and clean a single indicator from World Bank API\n",
    "    \"\"\"\n",
    "    # Construct the API URL\n",
    "    url = \"https://api.worldbank.org/v2/country/all/indicator/\" + indicator_code\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'mrnev': '1',\n",
    "        'per_page': '1000'\n",
    "    }\n",
    "    headers = {\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse JSON response\n",
    "        data = response.json()\n",
    "        \n",
    "        # The API returns an array where the second element contains the data\n",
    "        if len(data) < 2 or not data[1]:\n",
    "            print(f\"No data returned for indicator {indicator_code}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        records = data[1]\n",
    "        \n",
    "        # Extract relevant fields and create DataFrame\n",
    "        processed_records = []\n",
    "        for record in records:\n",
    "            if record and 'country' in record and 'date' in record:\n",
    "                processed_record = {\n",
    "                    'Country Code': record['country']['id'] if record['country'] else None,\n",
    "                    f'Year_{column_name}': record['date'],\n",
    "                    column_name: record['value']\n",
    "                }\n",
    "                processed_records.append(processed_record)\n",
    "        \n",
    "        df = pd.DataFrame(processed_records)\n",
    "        return df\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {indicator_code}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error processing data for {indicator_code}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame, numeric_cols: List[str], groupby_col: str, suffix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform min-max normalization within groups\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col not in df_copy.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate min and max for each group\n",
    "        bounds = df_copy.groupby(groupby_col)[col].agg(['min', 'max']).reset_index()\n",
    "        bounds.columns = [groupby_col, 'min_val', 'max_val']\n",
    "        \n",
    "        # Merge bounds back to main dataframe\n",
    "        df_copy = df_copy.merge(bounds, on=groupby_col, how='left')\n",
    "        \n",
    "        # Apply normalization\n",
    "        def normalize_value(row):\n",
    "            if pd.isna(row[col]) or pd.isna(row['min_val']) or pd.isna(row['max_val']):\n",
    "                return None\n",
    "            if row['min_val'] == row['max_val']:\n",
    "                return None\n",
    "            return (row[col] - row['min_val']) / (row['max_val'] - row['min_val'])\n",
    "        \n",
    "        df_copy[f'{col}_{suffix}'] = df_copy.apply(normalize_value, axis=1)\n",
    "        \n",
    "        # Remove temporary columns\n",
    "        df_copy = df_copy.drop(['min_val', 'max_val'], axis=1)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Infrastructure Quality indicator...\n",
      "Loaded 217 records\n",
      "Merging with country codes...\n",
      "Performing global normalization...\n",
      "Performing regional normalization...\n",
      "Performing income group normalization...\n",
      "Saving to CSV...\n",
      "Process completed successfully!\n",
      "Final dataset shape: (135, 13)\n",
      "\n",
      "First few rows:\n",
      "  code Year_InfraQuality  InfraQuality  InfraQuality_w  InfraQuality_r  \\\n",
      "0  AFG              2022           1.7        0.012876        0.000000   \n",
      "1  AGO              2022           2.1        0.184549        0.166667   \n",
      "2  ALB              2022           2.7        0.442060        0.533333   \n",
      "3  ARG              2022           2.8        0.484979        0.714286   \n",
      "4  ARM              2022           2.6        0.399142        0.466667   \n",
      "\n",
      "   InfraQuality_i      country    ifs                    imf_region  \\\n",
      "0        0.000000  Afghanistan  512.0                                 \n",
      "1        0.281046       Angola  614.0                        Africa   \n",
      "2        0.434783      Albania  914.0                        Europe   \n",
      "3        0.478261    Argentina  213.0            Western Hemisphere   \n",
      "4        0.391304      Armenia  911.0  Middle East and Central Asia   \n",
      "\n",
      "  imf_income code_2 country_unctad country_vcpe  \n",
      "0                AF    Afghanistan               \n",
      "1         EM     AO         Angola       Angola  \n",
      "2         EM     AL        Albania      Albania  \n",
      "3         EM     AR      Argentina    Argentina  \n",
      "4         EM     AM        Armenia      Armenia  \n"
     ]
    }
   ],
   "source": [
    "# Load the Infrastructure Quality indicator\n",
    "print(\"Loading Infrastructure Quality indicator...\")\n",
    "infra_quality = load_indicator(\"LP.LPI.INFR.XQ\", \"InfraQuality\")\n",
    "\n",
    "if infra_quality.empty:\n",
    "    print(\"Failed to load data. Exiting.\")\n",
    "    \n",
    "\n",
    "print(f\"Loaded {len(infra_quality)} records\")\n",
    "\n",
    "# Merge with country codes (Right Outer Join equivalent)\n",
    "print(\"Merging with country codes...\")\n",
    "merged_df = country_code_df.merge(\n",
    "    infra_quality, \n",
    "    left_on='code_2', \n",
    "    right_on='Country Code', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Define numeric indicators\n",
    "indicators = [\"InfraQuality\"]\n",
    "\n",
    "# Convert InfraQuality to numeric, handling any non-numeric values\n",
    "for col in indicators:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "\n",
    "print(\"Performing global normalization...\")\n",
    "# Global min-max normalization (suffix _w)\n",
    "result_df = merged_df.copy()\n",
    "for col in indicators:\n",
    "    if col in result_df.columns:\n",
    "        values = result_df[col].dropna()\n",
    "        if len(values) > 0:\n",
    "            min_val = values.min()\n",
    "            max_val = values.max()\n",
    "            if min_val != max_val:\n",
    "                result_df[f'{col}_w'] = (result_df[col] - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                result_df[f'{col}_w'] = None\n",
    "        else:\n",
    "            result_df[f'{col}_w'] = None\n",
    "\n",
    "print(\"Performing regional normalization...\")\n",
    "# Regional min-max normalization (suffix _r)\n",
    "result_df = normalize_columns(result_df, indicators, 'region', 'r')\n",
    "\n",
    "print(\"Performing income group normalization...\")\n",
    "# Income group min-max normalization (suffix _i)\n",
    "result_df = normalize_columns(result_df, indicators, 'incomegroup', 'i')\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_remove = ['Country Code', 'region', 'incomegroup']\n",
    "result_df = result_df.drop([col for col in columns_to_remove if col in result_df.columns], axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "desired_order = ['code', 'Year_InfraQuality', 'InfraQuality', 'InfraQuality_w', 'InfraQuality_r', 'InfraQuality_i']\n",
    "available_columns = [col for col in desired_order if col in result_df.columns]\n",
    "other_columns = [col for col in result_df.columns if col not in desired_order]\n",
    "final_column_order = available_columns + other_columns\n",
    "\n",
    "result_df = result_df[final_column_order]\n",
    "\n",
    "# Sort by code\n",
    "result_df = result_df.sort_values('code', ascending=True)\n",
    "\n",
    "# Reset index\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Saving to CSV...\")\n",
    "# Save to CSV\n",
    "result_df.to_csv('LPI.csv', index=False)\n",
    "\n",
    "print(\"Process completed successfully!\")\n",
    "print(f\"Final dataset shape: {result_df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPI_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indicator(indicator_code: str, column_name: str, page: str = \"1\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to load and clean a single indicator from World Bank API\n",
    "    and calculate change over time\n",
    "    \"\"\"\n",
    "    # Construct the API URL\n",
    "    url = \"https://api.worldbank.org/v2/country/all/indicator/\" + indicator_code\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'mrnev': '5',  # Get last 5 most recent values\n",
    "        'per_page': '1000',\n",
    "        'page': page\n",
    "    }\n",
    "    headers = {\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the API request\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse JSON response\n",
    "        data = response.json()\n",
    "        \n",
    "        # The API returns an array where the second element contains the data\n",
    "        if len(data) < 2 or not data[1]:\n",
    "            print(f\"No data returned for indicator {indicator_code}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        records = data[1]\n",
    "        \n",
    "        # Extract relevant fields and create DataFrame\n",
    "        processed_records = []\n",
    "        for record in records:\n",
    "            if record and 'country' in record and 'date' in record and record['value'] is not None:\n",
    "                processed_record = {\n",
    "                    'Country Code': record['country']['id'] if record['country'] else None,\n",
    "                    'Year': int(record['date']),\n",
    "                    column_name: float(record['value']) if record['value'] is not None else None\n",
    "                }\n",
    "                processed_records.append(processed_record)\n",
    "        \n",
    "        df = pd.DataFrame(processed_records)\n",
    "        \n",
    "        if df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Sort by Country Code and Year\n",
    "        df = df.sort_values(['Country Code', 'Year'])\n",
    "        \n",
    "        # Group by country and calculate change over time\n",
    "        def calculate_delta(group):\n",
    "            if len(group) < 2:\n",
    "                return pd.Series({\n",
    "                    'Country Code': group['Country Code'].iloc[0],\n",
    "                    f'd_{column_name}': None,\n",
    "                    f'Year Range_{column_name}': None\n",
    "                })\n",
    "            \n",
    "            # Remove rows with null values for the calculation\n",
    "            valid_data = group.dropna(subset=[column_name])\n",
    "            \n",
    "            if len(valid_data) < 2:\n",
    "                return pd.Series({\n",
    "                    'Country Code': group['Country Code'].iloc[0],\n",
    "                    f'd_{column_name}': None,\n",
    "                    f'Year Range_{column_name}': None\n",
    "                })\n",
    "            \n",
    "            first_year = valid_data['Year'].min()\n",
    "            last_year = valid_data['Year'].max()\n",
    "            start_val = valid_data[valid_data['Year'] == first_year][column_name].iloc[0]\n",
    "            end_val = valid_data[valid_data['Year'] == last_year][column_name].iloc[-1]\n",
    "            \n",
    "            # Calculate change per year\n",
    "            if last_year != first_year:\n",
    "                delta = (end_val - start_val) / (last_year - first_year)\n",
    "            else:\n",
    "                delta = None\n",
    "            \n",
    "            year_range = f\"{first_year}–{last_year}\"\n",
    "            \n",
    "            return pd.Series({\n",
    "                'Country Code': group['Country Code'].iloc[0],\n",
    "                f'd_{column_name}': delta,\n",
    "                f'Year Range_{column_name}': year_range\n",
    "            })\n",
    "        \n",
    "        # Apply the calculation\n",
    "        result = df.groupby('Country Code').apply(calculate_delta).reset_index(drop=True)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {indicator_code}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except (KeyError, IndexError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error processing data for {indicator_code}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame, numeric_cols: List[str], groupby_col: str, suffix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform min-max normalization within groups\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col not in df_copy.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate min and max for each group\n",
    "        bounds = df_copy.groupby(groupby_col)[col].agg(['min', 'max']).reset_index()\n",
    "        bounds.columns = [groupby_col, 'min_val', 'max_val']\n",
    "        \n",
    "        # Merge bounds back to main dataframe\n",
    "        df_copy = df_copy.merge(bounds, on=groupby_col, how='left')\n",
    "        \n",
    "        # Apply normalization\n",
    "        def normalize_value(row):\n",
    "            if pd.isna(row[col]) or pd.isna(row['min_val']) or pd.isna(row['max_val']):\n",
    "                return None\n",
    "            if row['min_val'] == row['max_val']:\n",
    "                return None\n",
    "            return (row[col] - row['min_val']) / (row['max_val'] - row['min_val'])\n",
    "        \n",
    "        df_copy[f'{col}_{suffix}'] = df_copy.apply(normalize_value, axis=1)\n",
    "        \n",
    "        # Remove temporary columns\n",
    "        df_copy = df_copy.drop(['min_val', 'max_val'], axis=1)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Infrastructure Quality indicator and calculating change over time...\n",
      "Loaded 206 country records\n",
      "Merging with country codes...\n",
      "Performing global normalization...\n",
      "Performing regional normalization...\n",
      "Performing income group normalization...\n",
      "Saving to CSV...\n",
      "Process completed successfully!\n",
      "Final dataset shape: (135, 13)\n",
      "\n",
      "First few rows:\n",
      "  code  d_InfraQuality Year Range_InfraQuality  d_InfraQuality_w  \\\n",
      "0  AFG       -0.030000               2012–2022          0.185138   \n",
      "1  AGO       -0.038000               2012–2022          0.135768   \n",
      "2  ALB        0.046667               2010–2022          0.658270   \n",
      "3  ARG       -0.014000               2012–2022          0.283879   \n",
      "4  ARM        0.022000               2012–2022          0.506045   \n",
      "\n",
      "   d_InfraQuality_r  d_InfraQuality_i      country    ifs  \\\n",
      "0          0.000000          0.185185  Afghanistan  512.0   \n",
      "1          0.135802          0.132046       Angola  614.0   \n",
      "2          0.647390          0.653519      Albania  914.0   \n",
      "3          0.175182          0.273921    Argentina  213.0   \n",
      "4          0.490318          0.499177      Armenia  911.0   \n",
      "\n",
      "                     imf_region imf_income code_2 country_unctad country_vcpe  \n",
      "0                                              AF    Afghanistan               \n",
      "1                        Africa         EM     AO         Angola       Angola  \n",
      "2                        Europe         EM     AL        Albania      Albania  \n",
      "3            Western Hemisphere         EM     AR      Argentina    Argentina  \n",
      "4  Middle East and Central Asia         EM     AM        Armenia      Armenia  \n",
      "\n",
      "Delta InfraQuality Statistics:\n",
      "count    106.000000\n",
      "mean       0.010639\n",
      "std        0.036630\n",
      "min       -0.060000\n",
      "25%       -0.010000\n",
      "50%        0.008875\n",
      "75%        0.033833\n",
      "max        0.102041\n",
      "Name: d_InfraQuality, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\1871963053.py:91: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result = df.groupby('Country Code').apply(calculate_delta).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Load the Infrastructure Quality indicator with change calculation\n",
    "print(\"Loading Infrastructure Quality indicator and calculating change over time...\")\n",
    "infra_quality = load_indicator(\"LP.LPI.INFR.XQ\", \"InfraQuality\", \"1\")\n",
    "\n",
    "if infra_quality.empty:\n",
    "    print(\"Failed to load data. Exiting.\")\n",
    "    \n",
    "\n",
    "print(f\"Loaded {len(infra_quality)} country records\")\n",
    "\n",
    "# Merge with country codes (Right Outer Join equivalent)\n",
    "print(\"Merging with country codes...\")\n",
    "merged_df = country_code_df.merge(\n",
    "    infra_quality, \n",
    "    left_on='code_2', \n",
    "    right_on='Country Code', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Define numeric indicators (delta columns)\n",
    "indicators = [\"d_InfraQuality\"]\n",
    "\n",
    "# Convert delta column to numeric, handling any non-numeric values\n",
    "for col in indicators:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "\n",
    "print(\"Performing global normalization...\")\n",
    "# Global min-max normalization (suffix _w)\n",
    "result_df = merged_df.copy()\n",
    "for col in indicators:\n",
    "    if col in result_df.columns:\n",
    "        values = result_df[col].dropna()\n",
    "        if len(values) > 0:\n",
    "            min_val = values.min()\n",
    "            max_val = values.max()\n",
    "            if min_val != max_val:\n",
    "                result_df[f'{col}_w'] = (result_df[col] - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                result_df[f'{col}_w'] = None\n",
    "        else:\n",
    "            result_df[f'{col}_w'] = None\n",
    "\n",
    "print(\"Performing regional normalization...\")\n",
    "# Regional min-max normalization (suffix _r)\n",
    "result_df = normalize_columns(result_df, indicators, 'region', 'r')\n",
    "\n",
    "print(\"Performing income group normalization...\")\n",
    "# Income group min-max normalization (suffix _i)\n",
    "result_df = normalize_columns(result_df, indicators, 'incomegroup', 'i')\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_remove = ['Country Code', 'region', 'incomegroup']\n",
    "result_df = result_df.drop([col for col in columns_to_remove if col in result_df.columns], axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "desired_order = ['code', 'd_InfraQuality', 'Year Range_InfraQuality', 'd_InfraQuality_w', 'd_InfraQuality_r', 'd_InfraQuality_i']\n",
    "available_columns = [col for col in desired_order if col in result_df.columns]\n",
    "other_columns = [col for col in result_df.columns if col not in desired_order]\n",
    "final_column_order = available_columns + other_columns\n",
    "\n",
    "result_df = result_df[final_column_order]\n",
    "\n",
    "# Sort by code\n",
    "result_df = result_df.sort_values('code', ascending=True)\n",
    "\n",
    "# Reset index\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "print(\"Saving to CSV...\")\n",
    "# Save to CSV\n",
    "result_df.to_csv('LPI_d.csv', index=False)\n",
    "\n",
    "print(\"Process completed successfully!\")\n",
    "print(f\"Final dataset shape: {result_df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result_df.head())\n",
    "\n",
    "# Show some statistics about the delta values\n",
    "if 'd_InfraQuality' in result_df.columns:\n",
    "    delta_stats = result_df['d_InfraQuality'].describe()\n",
    "    print(f\"\\nDelta InfraQuality Statistics:\")\n",
    "    print(delta_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_category(category_code, category_label):\n",
    "    \"\"\"\n",
    "    Load PCI data for a specific category from UNCTAD API\n",
    "    \"\"\"\n",
    "    \n",
    "    year_list = list(range(2000, 2101))\n",
    "    year_list_text = \",\".join(map(str, year_list))\n",
    "    \n",
    "    # Filter\n",
    "    filter_clause = f\"Category/Code eq '{category_code}' and Year in ({year_list_text})\"\n",
    "    \n",
    "    # Form body\n",
    "    compute_clause = (\n",
    "        \"round(M6080/Value div 1, 1) as Productive_Capacities_Index_Value,\"\n",
    "        \"M6080/Footnote/Text as Productive_Capacities_Index_Footnote,\"\n",
    "        \"M6080/MissingValue/Label as Productive_Capacities_Index_MissingValue\"\n",
    "    )\n",
    "    \n",
    "    form_body = (\n",
    "        \"$select=Economy/Label ,Year,\"\n",
    "        \"Productive_Capacities_Index_Value,\"\n",
    "        \"Productive_Capacities_Index_Footnote,\"\n",
    "        \"Productive_Capacities_Index_MissingValue\"\n",
    "        f\"&$filter={quote(filter_clause)}\"\n",
    "        \"&$orderby=Economy/Order asc ,Year asc\"\n",
    "        f\"&$compute={quote(compute_clause)}\"\n",
    "        \"&$format=csv\"\n",
    "        \"&compress=gz\"\n",
    "    )\n",
    "    \n",
    "    # API call\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'ClientId': client_id,\n",
    "        'ClientSecret': client_secret\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://unctadstat-user-api.unctad.org/US.PCI/cur/Facts?culture=en\",\n",
    "            data=form_body,\n",
    "            headers=headers\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Decompress and parse\n",
    "        decompressed_data = gzip.decompress(response.content)\n",
    "        csv_data = io.StringIO(decompressed_data.decode('utf-8'))\n",
    "        \n",
    "        # Read CSV\n",
    "        df = pd.read_csv(csv_data)\n",
    "        \n",
    "        # Keep only required columns\n",
    "        required_cols = ['Economy_Label', 'Year', 'Productive_Capacities_Index_Value']\n",
    "        df = df[required_cols]\n",
    "        \n",
    "        # Group by Economy_Label and get the row with maximum Year\n",
    "        latest_data = df.loc[df.groupby('Economy_Label')['Year'].idxmax()]\n",
    "        \n",
    "        # Rename columns\n",
    "        latest_data = latest_data.rename(columns={\n",
    "            'Productive_Capacities_Index_Value': category_label,\n",
    "            'Year': f'Year_{category_label}'\n",
    "        })\n",
    "        \n",
    "        return latest_data[['Economy_Label', f'Year_{category_label}', category_label]]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading category {category_code} ({category_label}): {e}\")\n",
    "        # Return empty dataframe with expected structure\n",
    "        return pd.DataFrame(columns=['Economy_Label', f'Year_{category_label}', category_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PCI data categories...\n",
      "Merging categories...\n",
      "Joining with country codes...\n",
      "Converting to numeric types...\n",
      "Applying global normalization...\n",
      "Applying regional normalization...\n",
      "Applying income group normalization...\n",
      "Finalizing dataframe...\n",
      "Saving to PCI.csv...\n",
      "PCI.csv saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load and rename each category with its own year column\n",
    "print(\"Loading PCI data categories...\")\n",
    "overall_pci = load_category(\"00\", \"Overall PCI\")\n",
    "human_capital = load_category(\"30\", \"Human Capital\")\n",
    "natural_capital = load_category(\"50\", \"Natural Capital\")\n",
    "ict = load_category(\"60\", \"ICT\")\n",
    "transport = load_category(\"70\", \"Transport\")\n",
    "private_sector = load_category(\"80\", \"Private Sector\")\n",
    "\n",
    "print(\"Merging categories...\")\n",
    "# Merge all on Economy_Label\n",
    "merged_df = overall_pci.copy()\n",
    "\n",
    "# Sequential left joins\n",
    "for df in [human_capital, natural_capital, ict, transport, private_sector]:\n",
    "    merged_df = merged_df.merge(df, on='Economy_Label', how='left')\n",
    "\n",
    "# Join with country codes using right outer join\n",
    "print(\"Joining with country codes...\")\n",
    "result = country_code_df.merge(\n",
    "    merged_df, \n",
    "    left_on='country_unctad', \n",
    "    right_on='Economy_Label', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Define indicators list\n",
    "indicators = [\"Overall PCI\", \"Human Capital\", \"Natural Capital\", \"ICT\", \"Transport\", \"Private Sector\"]\n",
    "\n",
    "# Convert indicator columns to numeric\n",
    "print(\"Converting to numeric types...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        result[indicator] = pd.to_numeric(result[indicator], errors='coerce')\n",
    "\n",
    "# Global min-max normalization (suffix _w)\n",
    "print(\"Applying global normalization...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        values = result[indicator].dropna()\n",
    "        if len(values) > 0:\n",
    "            min_val = values.min()\n",
    "            max_val = values.max()\n",
    "            if min_val != max_val:\n",
    "                result[f'{indicator}_w'] = (result[indicator] - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                result[f'{indicator}_w'] = None\n",
    "        else:\n",
    "            result[f'{indicator}_w'] = None\n",
    "\n",
    "# Regional min-max normalization (suffix _r)\n",
    "print(\"Applying regional normalization...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        regional_norms = []\n",
    "        for _, group in result.groupby('region'):\n",
    "            values = group[indicator].dropna()\n",
    "            if len(values) > 0:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                if min_val != max_val:\n",
    "                    group_norm = (group[indicator] - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            else:\n",
    "                group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            regional_norms.append(group_norm)\n",
    "        \n",
    "        result[f'{indicator}_r'] = pd.concat(regional_norms).reindex(result.index)\n",
    "\n",
    "# Income group min-max normalization (suffix _i)\n",
    "print(\"Applying income group normalization...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        income_norms = []\n",
    "        for _, group in result.groupby('incomegroup'):\n",
    "            values = group[indicator].dropna()\n",
    "            if len(values) > 0:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                if min_val != max_val:\n",
    "                    group_norm = (group[indicator] - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            else:\n",
    "                group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            income_norms.append(group_norm)\n",
    "        \n",
    "        result[f'{indicator}_i'] = pd.concat(income_norms).reindex(result.index)\n",
    "\n",
    "# Remove columns and reorder\n",
    "print(\"Finalizing dataframe...\")\n",
    "columns_to_remove = ['Economy_Label', 'region', 'incomegroup']\n",
    "for col in columns_to_remove:\n",
    "    if col in result.columns:\n",
    "        result = result.drop(columns=[col])\n",
    "\n",
    "# Define the desired column order\n",
    "year_cols = [f\"Year_{indicator}\" for indicator in indicators]\n",
    "indicator_cols = indicators\n",
    "w_cols = [f\"{indicator}_w\" for indicator in indicators]\n",
    "r_cols = [f\"{indicator}_r\" for indicator in indicators]\n",
    "i_cols = [f\"{indicator}_i\" for indicator in indicators]\n",
    "\n",
    "# Build the final column order\n",
    "final_columns = ['code'] + year_cols + indicator_cols + w_cols + r_cols + i_cols\n",
    "\n",
    "# Only include columns that actually exist in the dataframe\n",
    "existing_columns = [col for col in final_columns if col in result.columns]\n",
    "result = result[existing_columns]\n",
    "\n",
    "# Sort by code\n",
    "result = result.sort_values('code').reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "print(\"Saving to PCI.csv...\")\n",
    "result.to_csv('PCI.csv', index=False)\n",
    "print(\"PCI.csv saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCI_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_category(category_code, category_label):\n",
    "    \"\"\"\n",
    "    Load PCI data for a specific category from UNCTAD API and calculate change over time\n",
    "    \"\"\"\n",
    "    \n",
    "    year_list = list(range(2000, 2101))\n",
    "    year_list_text = \",\".join(map(str, year_list))\n",
    "    \n",
    "    # Filter\n",
    "    filter_clause = f\"Category/Code eq '{category_code}' and Year in ({year_list_text})\"\n",
    "    \n",
    "    # Form body\n",
    "    compute_clause = (\n",
    "        \"round(M6080/Value div 1, 1) as Productive_Capacities_Index_Value,\"\n",
    "        \"M6080/Footnote/Text as Productive_Capacities_Index_Footnote,\"\n",
    "        \"M6080/MissingValue/Label as Productive_Capacities_Index_MissingValue\"\n",
    "    )\n",
    "    \n",
    "    form_body = (\n",
    "        \"$select=Economy/Label ,Year,\"\n",
    "        \"Productive_Capacities_Index_Value,\"\n",
    "        \"Productive_Capacities_Index_Footnote,\"\n",
    "        \"Productive_Capacities_Index_MissingValue\"\n",
    "        f\"&$filter={quote(filter_clause)}\"\n",
    "        \"&$orderby=Economy/Order asc ,Year asc\"\n",
    "        f\"&$compute={quote(compute_clause)}\"\n",
    "        \"&$format=csv\"\n",
    "        \"&compress=gz\"\n",
    "    )\n",
    "    \n",
    "    # API call\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'ClientId': client_id,\n",
    "        'ClientSecret': client_secret\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://unctadstat-user-api.unctad.org/US.PCI/cur/Facts?culture=en\",\n",
    "            data=form_body,\n",
    "            headers=headers\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Decompress and parse\n",
    "        decompressed_data = gzip.decompress(response.content)\n",
    "        csv_data = io.StringIO(decompressed_data.decode('utf-8'))\n",
    "        \n",
    "        # Read CSV\n",
    "        df = pd.read_csv(csv_data)\n",
    "        \n",
    "        # Keep only required columns and ensure proper types\n",
    "        required_cols = ['Economy_Label', 'Year', 'Productive_Capacities_Index_Value']\n",
    "        df = df[required_cols].copy()\n",
    "        df['Year'] = df['Year'].astype('int64')\n",
    "        \n",
    "        # Remove rows with null values\n",
    "        df_clean = df.dropna(subset=['Productive_Capacities_Index_Value']).copy()\n",
    "        \n",
    "        # Find the latest year across all countries\n",
    "        latest_year = df_clean['Year'].max()\n",
    "        cut_year = latest_year - 4\n",
    "        \n",
    "        # Keep only rows from the last 5 years\n",
    "        window_5 = df_clean[df_clean['Year'] >= cut_year].copy()\n",
    "        \n",
    "        # Sort by country and year\n",
    "        window_5 = window_5.sort_values(['Economy_Label', 'Year'])\n",
    "        \n",
    "        # Group by Economy_Label and calculate summary statistics\n",
    "        def calculate_stats(group):\n",
    "            years = group['Year'].values\n",
    "            values = group['Productive_Capacities_Index_Value'].values\n",
    "            \n",
    "            return pd.Series({\n",
    "                'FirstYear': years.min(),\n",
    "                'LastYear': years.max(),\n",
    "                'StartVal': values[0] if len(values) > 0 else None,\n",
    "                'EndVal': values[-1] if len(values) > 0 else None\n",
    "            })\n",
    "        \n",
    "        summarized = window_5.groupby('Economy_Label').apply(calculate_stats).reset_index()\n",
    "        \n",
    "        # Ensure proper types\n",
    "        summarized['FirstYear'] = summarized['FirstYear'].astype('int64')\n",
    "        summarized['LastYear'] = summarized['LastYear'].astype('int64')\n",
    "        summarized['StartVal'] = pd.to_numeric(summarized['StartVal'], errors='coerce')\n",
    "        summarized['EndVal'] = pd.to_numeric(summarized['EndVal'], errors='coerce')\n",
    "        \n",
    "        # Add column for change over time (derivative)\n",
    "        summarized[f'd_{category_label}'] = (\n",
    "            (summarized['EndVal'] - summarized['StartVal']) / \n",
    "            (summarized['LastYear'] - summarized['FirstYear'])\n",
    "        )\n",
    "        \n",
    "        # Add year range column\n",
    "        summarized[f'Year Range_{category_label}'] = (\n",
    "            summarized['FirstYear'].astype(str) + '–' + summarized['LastYear'].astype(str)\n",
    "        )\n",
    "        \n",
    "        # Select final columns\n",
    "        final_columns = ['Economy_Label', f'd_{category_label}', f'Year Range_{category_label}']\n",
    "        return summarized[final_columns]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading category {category_code} ({category_label}): {e}\")\n",
    "        # Return empty dataframe with expected structure\n",
    "        return pd.DataFrame(columns=['Economy_Label', f'd_{category_label}', f'Year Range_{category_label}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PCI data categories and calculating changes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\1269043781.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summarized = window_5.groupby('Economy_Label').apply(calculate_stats).reset_index()\n",
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\1269043781.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summarized = window_5.groupby('Economy_Label').apply(calculate_stats).reset_index()\n",
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\1269043781.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summarized = window_5.groupby('Economy_Label').apply(calculate_stats).reset_index()\n",
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\1269043781.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summarized = window_5.groupby('Economy_Label').apply(calculate_stats).reset_index()\n",
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\1269043781.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summarized = window_5.groupby('Economy_Label').apply(calculate_stats).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging categories...\n",
      "Joining with country codes...\n",
      "Converting to numeric types...\n",
      "Applying global normalization...\n",
      "Applying regional normalization...\n",
      "Applying income group normalization...\n",
      "Finalizing dataframe...\n",
      "Saving to PCI.csv...\n",
      "PCI.csv saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esn\\AppData\\Local\\Temp\\ipykernel_8988\\1269043781.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summarized = window_5.groupby('Economy_Label').apply(calculate_stats).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Load and rename each category with change calculations\n",
    "print(\"Loading PCI data categories and calculating changes...\")\n",
    "overall_pci = load_category(\"00\", \"Overall PCI\")\n",
    "human_capital = load_category(\"30\", \"Human Capital\")\n",
    "natural_capital = load_category(\"50\", \"Natural Capital\")\n",
    "ict = load_category(\"60\", \"ICT\")\n",
    "transport = load_category(\"70\", \"Transport\")\n",
    "private_sector = load_category(\"80\", \"Private Sector\")\n",
    "\n",
    "print(\"Merging categories...\")\n",
    "# Merge all on Economy_Label\n",
    "merged_df = overall_pci.copy()\n",
    "\n",
    "# Sequential left joins\n",
    "for df in [human_capital, natural_capital, ict, transport, private_sector]:\n",
    "    merged_df = merged_df.merge(df, on='Economy_Label', how='left')\n",
    "\n",
    "# Join with country codes using right outer join\n",
    "print(\"Joining with country codes...\")\n",
    "result = country_code_df.merge(\n",
    "    merged_df, \n",
    "    left_on='country_unctad', \n",
    "    right_on='Economy_Label', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Define indicators list (change indicators)\n",
    "indicators = [\"d_Overall PCI\", \"d_Human Capital\", \"d_Natural Capital\", \"d_ICT\", \"d_Transport\", \"d_Private Sector\"]\n",
    "\n",
    "# Convert indicator columns to numeric\n",
    "print(\"Converting to numeric types...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        result[indicator] = pd.to_numeric(result[indicator], errors='coerce')\n",
    "\n",
    "# Global min-max normalization (suffix _w)\n",
    "print(\"Applying global normalization...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        values = result[indicator].dropna()\n",
    "        if len(values) > 0:\n",
    "            min_val = values.min()\n",
    "            max_val = values.max()\n",
    "            if min_val != max_val:\n",
    "                result[f'{indicator}_w'] = (result[indicator] - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                result[f'{indicator}_w'] = None\n",
    "        else:\n",
    "            result[f'{indicator}_w'] = None\n",
    "\n",
    "# Regional min-max normalization (suffix _r)\n",
    "print(\"Applying regional normalization...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        regional_norms = []\n",
    "        for _, group in result.groupby('region'):\n",
    "            values = group[indicator].dropna()\n",
    "            if len(values) > 0:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                if min_val != max_val:\n",
    "                    group_norm = (group[indicator] - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            else:\n",
    "                group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            regional_norms.append(group_norm)\n",
    "        \n",
    "        result[f'{indicator}_r'] = pd.concat(regional_norms).reindex(result.index)\n",
    "\n",
    "# Income group min-max normalization (suffix _i)\n",
    "print(\"Applying income group normalization...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        income_norms = []\n",
    "        for _, group in result.groupby('incomegroup'):\n",
    "            values = group[indicator].dropna()\n",
    "            if len(values) > 0:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                if min_val != max_val:\n",
    "                    group_norm = (group[indicator] - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            else:\n",
    "                group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            income_norms.append(group_norm)\n",
    "        \n",
    "        result[f'{indicator}_i'] = pd.concat(income_norms).reindex(result.index)\n",
    "\n",
    "# Remove columns and reorder\n",
    "print(\"Finalizing dataframe...\")\n",
    "columns_to_remove = ['region', 'incomegroup', 'Economy_Label']\n",
    "for col in columns_to_remove:\n",
    "    if col in result.columns:\n",
    "        result = result.drop(columns=[col])\n",
    "\n",
    "# Define the desired column order based on the Power Query\n",
    "base_indicators = [\"Overall PCI\", \"Human Capital\", \"Natural Capital\", \"ICT\", \"Transport\", \"Private Sector\"]\n",
    "\n",
    "# Build column lists\n",
    "d_cols = [f\"d_{indicator}\" for indicator in base_indicators]\n",
    "year_range_cols = [f\"Year Range_{indicator}\" for indicator in base_indicators]\n",
    "w_cols = [f\"d_{indicator}_w\" for indicator in base_indicators]\n",
    "r_cols = [f\"d_{indicator}_r\" for indicator in base_indicators]\n",
    "i_cols = [f\"d_{indicator}_i\" for indicator in base_indicators]\n",
    "\n",
    "# Interleave d_ and Year Range_ columns as shown in the Power Query reordering\n",
    "interleaved_cols = []\n",
    "for i, indicator in enumerate(base_indicators):\n",
    "    if i == 0:  # First indicator gets d_ first\n",
    "        interleaved_cols.extend([f\"d_{indicator}\", f\"Year Range_{indicator}\"])\n",
    "    else:  # Others get Year Range_ first\n",
    "        interleaved_cols.extend([f\"Year Range_{indicator}\", f\"d_{indicator}\"])\n",
    "\n",
    "# Build the final column order\n",
    "final_columns = ['code'] + interleaved_cols + w_cols + r_cols + i_cols\n",
    "\n",
    "# Only include columns that actually exist in the dataframe\n",
    "existing_columns = [col for col in final_columns if col in result.columns]\n",
    "result = result[existing_columns]\n",
    "\n",
    "# Sort by code\n",
    "result = result.sort_values('code').reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "print(\"Saving to PCI.csv...\")\n",
    "result.to_csv('PCI.csv', index=False)\n",
    "print(\"PCI.csv saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fdi_pct(pct_type):\n",
    "    \"\"\"\n",
    "    Load FDI inflow percentage indicator (GDP or GFCF) for economies, 1990-2023.\n",
    "    \n",
    "    Args:\n",
    "        pct_type (str): \"GDP\" or \"GFCF\"\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: FDI data with latest values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Year list\n",
    "    year_list = list(range(1990, 2101))\n",
    "    year_text = \",\".join(map(str, year_list))\n",
    "    \n",
    "    # Pick the right measure & field names\n",
    "    pct_type_upper = pct_type.upper()\n",
    "    if pct_type_upper == \"GDP\":\n",
    "        measure_record = {\n",
    "            'MCode': 'M5025',\n",
    "            'SelName': 'Percentage_of_Gross_Domestic_Product',\n",
    "            'OutColName': 'FDI inflows (% GDP)'\n",
    "        }\n",
    "    elif pct_type_upper == \"GFCF\":\n",
    "        measure_record = {\n",
    "            'MCode': 'M5026',\n",
    "            'SelName': 'Percentage_of_Gross_Fixed_Capital_Formation',\n",
    "            'OutColName': 'FDI inflows (% GFCF)'\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError('pct_type must be \"GDP\" or \"GFCF\"')\n",
    "    \n",
    "    # Build filter clause\n",
    "    filter_clause = f\"Direction/Code eq 1 and Flow/Code eq '09' and Year in ({year_text})\"\n",
    "    \n",
    "    # Form body\n",
    "    compute_clause = f\"round({measure_record['MCode']}/Value div 1, 2) as {measure_record['SelName']}_Value\"\n",
    "    \n",
    "    form_body = (\n",
    "        f\"$select=Economy/Label ,Year,{measure_record['SelName']}_Value\"\n",
    "        f\"&$filter={quote(filter_clause)}\"\n",
    "        \"&$orderby=Economy/Order asc ,Year asc\"\n",
    "        f\"&$compute={quote(compute_clause)}\"\n",
    "        \"&$format=csv&compress=gz\"\n",
    "    )\n",
    "    \n",
    "    # API call\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'ClientId': client_id,\n",
    "        'ClientSecret': client_secret\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://unctadstat-user-api.unctad.org/US.FdiFlowsStock/cur/Facts?culture=en\",\n",
    "            data=form_body,\n",
    "            headers=headers\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Decompress and parse\n",
    "        decompressed_data = gzip.decompress(response.content)\n",
    "        csv_data = io.StringIO(decompressed_data.decode('utf-8'))\n",
    "        \n",
    "        # Read CSV\n",
    "        df = pd.read_csv(csv_data)\n",
    "        \n",
    "        # Keep only required columns\n",
    "        value_col = f\"{measure_record['SelName']}_Value\"\n",
    "        required_cols = ['Economy_Label', 'Year', value_col]\n",
    "        df = df[required_cols]\n",
    "        \n",
    "        # Group by Economy_Label and get the row with maximum Year\n",
    "        latest_data = df.loc[df.groupby('Economy_Label')['Year'].idxmax()]\n",
    "        \n",
    "        # Rename columns\n",
    "        latest_data = latest_data.rename(columns={\n",
    "            value_col: measure_record['SelName'],\n",
    "            'Year': f\"Year_{measure_record['SelName']}\"\n",
    "        })\n",
    "        \n",
    "        return latest_data[['Economy_Label', f\"Year_{measure_record['SelName']}\", measure_record['SelName']]]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FDI {pct_type} data: {e}\")\n",
    "        # Return empty dataframe with expected structure\n",
    "        return pd.DataFrame(columns=['Economy_Label', f\"Year_{measure_record['SelName']}\", measure_record['SelName']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FDI data...\n",
      "Merging FDI datasets...\n",
      "Joining with country codes...\n",
      "Converting to numeric types...\n",
      "Applying global normalization...\n",
      "Applying regional normalization...\n",
      "Applying income group normalization...\n",
      "Finalizing dataframe...\n",
      "Saving to FDI.csv...\n",
      "FDI.csv saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load FDI data for both GDP and GFCF\n",
    "print(\"Loading FDI data...\")\n",
    "fdi_pct_gdp_raw = load_fdi_pct(\"GDP\")\n",
    "fdi_pct_gfcf_raw = load_fdi_pct(\"GFCF\")\n",
    "\n",
    "print(\"Merging FDI datasets...\")\n",
    "# Merge on Economy_Label using full outer join\n",
    "merged = fdi_pct_gdp_raw.merge(\n",
    "    fdi_pct_gfcf_raw, \n",
    "    on='Economy_Label', \n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Join with country codes using right outer join\n",
    "print(\"Joining with country codes...\")\n",
    "result = country_code_df.merge(\n",
    "    merged, \n",
    "    left_on='country_unctad', \n",
    "    right_on='Economy_Label', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Define indicators list\n",
    "indicators = [\"Percentage_of_Gross_Domestic_Product\", \"Percentage_of_Gross_Fixed_Capital_Formation\"]\n",
    "\n",
    "# Convert indicator columns to numeric\n",
    "print(\"Converting to numeric types...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        result[indicator] = pd.to_numeric(result[indicator], errors='coerce')\n",
    "\n",
    "# Global min-max normalization (suffix _w)\n",
    "print(\"Applying global normalization...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        values = result[indicator].dropna()\n",
    "        if len(values) > 0:\n",
    "            min_val = values.min()\n",
    "            max_val = values.max()\n",
    "            if min_val != max_val:\n",
    "                result[f'{indicator}_w'] = (result[indicator] - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                result[f'{indicator}_w'] = None\n",
    "        else:\n",
    "            result[f'{indicator}_w'] = None\n",
    "\n",
    "# Regional min-max normalization (suffix _r)\n",
    "print(\"Applying regional normalization...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        regional_norms = []\n",
    "        for _, group in result.groupby('region'):\n",
    "            values = group[indicator].dropna()\n",
    "            if len(values) > 0:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                if min_val != max_val:\n",
    "                    group_norm = (group[indicator] - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            else:\n",
    "                group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            regional_norms.append(group_norm)\n",
    "        \n",
    "        result[f'{indicator}_r'] = pd.concat(regional_norms).reindex(result.index)\n",
    "\n",
    "# Income group min-max normalization (suffix _i)\n",
    "print(\"Applying income group normalization...\")\n",
    "for indicator in indicators:\n",
    "    if indicator in result.columns:\n",
    "        income_norms = []\n",
    "        for _, group in result.groupby('incomegroup'):\n",
    "            values = group[indicator].dropna()\n",
    "            if len(values) > 0:\n",
    "                min_val = values.min()\n",
    "                max_val = values.max()\n",
    "                if min_val != max_val:\n",
    "                    group_norm = (group[indicator] - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            else:\n",
    "                group_norm = pd.Series([None] * len(group), index=group.index)\n",
    "            income_norms.append(group_norm)\n",
    "        \n",
    "        result[f'{indicator}_i'] = pd.concat(income_norms).reindex(result.index)\n",
    "\n",
    "# Remove columns and reorder\n",
    "print(\"Finalizing dataframe...\")\n",
    "columns_to_remove = ['Economy_Label', 'region', 'incomegroup']\n",
    "for col in columns_to_remove:\n",
    "    if col in result.columns:\n",
    "        result = result.drop(columns=[col])\n",
    "\n",
    "# Define the desired column order based on the Power Query reordering\n",
    "final_columns = [\n",
    "    'code',\n",
    "    'Year_Percentage_of_Gross_Domestic_Product',\n",
    "    'Percentage_of_Gross_Domestic_Product',\n",
    "    'Year_Percentage_of_Gross_Fixed_Capital_Formation',\n",
    "    'Percentage_of_Gross_Fixed_Capital_Formation',\n",
    "    'Percentage_of_Gross_Domestic_Product_w',\n",
    "    'Percentage_of_Gross_Fixed_Capital_Formation_w',\n",
    "    'Percentage_of_Gross_Domestic_Product_r',\n",
    "    'Percentage_of_Gross_Fixed_Capital_Formation_r',\n",
    "    'Percentage_of_Gross_Domestic_Product_i',\n",
    "    'Percentage_of_Gross_Fixed_Capital_Formation_i'\n",
    "]\n",
    "\n",
    "# Only include columns that actually exist in the dataframe\n",
    "existing_columns = [col for col in final_columns if col in result.columns]\n",
    "result = result[existing_columns]\n",
    "\n",
    "# Sort by code\n",
    "result = result.sort_values('code').reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "print(\"Saving to FDI.csv...\")\n",
    "result.to_csv('FDI.csv', index=False)\n",
    "print(\"FDI.csv saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDI_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 1: Helper function to fetch and process FDI data ----\n",
    "\n",
    "def load_fdi_pct(pct_type):\n",
    "\n",
    "    years = list(range(1990, 2101))\n",
    "    year_text = \",\".join(str(y) for y in years)\n",
    "\n",
    "    if pct_type.upper() == \"GDP\":\n",
    "        mcode = \"M5025\"\n",
    "        sel_name = \"Percentage_of_Gross_Domestic_Product\"\n",
    "        out_col = \"FDI inflows (% GDP)\"\n",
    "    elif pct_type.upper() == \"GFCF\":\n",
    "        mcode = \"M5026\"\n",
    "        sel_name = \"Percentage_of_Gross_Fixed_Capital_Formation\"\n",
    "        out_col = \"FDI inflows (% GFCF)\"\n",
    "    else:\n",
    "        raise ValueError(\"pct_type must be 'GDP' or 'GFCF'\")\n",
    "\n",
    "    filter_clause = (\n",
    "        \"Direction/Code eq 1 and Flow/Code eq '09' and Year in ({})\".format(year_text)\n",
    "    )\n",
    "    form_body = (\n",
    "        \"$select=Economy/Label ,Year,\" + sel_name + \"_Value\"\n",
    "        + \"&$filter=\"  + requests.utils.quote(filter_clause)\n",
    "        + \"&$orderby=Economy/Order asc ,Year asc\"\n",
    "        + \"&$compute=\" + requests.utils.quote(\n",
    "            f\"round({mcode}/Value div 1, 2) as {sel_name}_Value\")\n",
    "        + \"&$format=csv&compress=gz\"\n",
    "    )\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        \"ClientId\": client_id,\n",
    "        \"ClientSecret\": client_secret\n",
    "    }\n",
    "\n",
    "    resp = requests.post(\n",
    "        \"https://unctadstat-user-api.unctad.org/US.FdiFlowsStock/cur/Facts?culture=en\",\n",
    "        data=form_body,\n",
    "        headers=headers\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    with gzip.GzipFile(fileobj=BytesIO(resp.content)) as gz:\n",
    "        df = pd.read_csv(gz)\n",
    "\n",
    "    # Drop rows with null FDI values\n",
    "    cat_label = sel_name + \"_Value\"\n",
    "    df = df.dropna(subset=[cat_label])\n",
    "    # Only last 5 years per country\n",
    "    latest_year = df['Year'].max()\n",
    "    cut_year = latest_year - 4\n",
    "    df = df[df['Year'] >= cut_year]\n",
    "    df = df.sort_values(['Economy_Label', 'Year'])\n",
    "\n",
    "    # Summarize: first/last year, first/last value, rate of change\n",
    "    summary = (\n",
    "        df.groupby(\"Economy_Label\")\n",
    "        .agg(\n",
    "            FirstYear=('Year', 'min'),\n",
    "            LastYear=('Year', 'max'),\n",
    "            StartVal=(cat_label, 'first'),\n",
    "            EndVal=(cat_label, 'last')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    summary[f'd_{cat_label}'] = (summary['EndVal'] - summary['StartVal']) / (summary['LastYear'] - summary['FirstYear'])\n",
    "    summary[f'Year Range_{cat_label}'] = summary['FirstYear'].astype(str) + \"–\" + summary['LastYear'].astype(str)\n",
    "    out = summary[[\"Economy_Label\", f'd_{cat_label}', f'Year Range_{cat_label}']]\n",
    "    return out\n",
    "\n",
    "# ---- Step 2: Fetch GDP and GFCF, merge on Economy_Label ----\n",
    "\n",
    "fdi_pct_gdp_raw = load_fdi_pct(\"GDP\")\n",
    "fdi_pct_gfcf_raw = load_fdi_pct(\"GFCF\")\n",
    "\n",
    "merged = pd.merge(\n",
    "    fdi_pct_gdp_raw, fdi_pct_gfcf_raw,\n",
    "    on=\"Economy_Label\", how=\"outer\",\n",
    "    suffixes=(\"\", \"_GFCF\")\n",
    ")\n",
    "\n",
    "# ---- Step 3: Merge with your country_code_df on UNCTAD country label ----\n",
    "\n",
    "# Suppose your country_code_df is already loaded\n",
    "# country_code_df = pd.read_csv(\"country_code_df.csv\")\n",
    "merged2 = pd.merge(\n",
    "    merged, country_code_df,\n",
    "    left_on=\"Economy_Label\", right_on=\"country_unctad\", how=\"right\"\n",
    ")\n",
    "\n",
    "# ---- Step 4: Rename columns for clarity ----\n",
    "\n",
    "merged2 = merged2.rename(columns={\n",
    "    'd_Percentage_of_Gross_Domestic_Product_Value': 'd_Percentage_of_Gross_Domestic_Product',\n",
    "    'Year Range_Percentage_of_Gross_Domestic_Product_Value': 'Year Range_Percentage_of_Gross_Domestic_Product',\n",
    "    'd_Percentage_of_Gross_Fixed_Capital_Formation_Value': 'd_Percentage_of_Gross_Fixed_Capital_Formation',\n",
    "    'Year Range_Percentage_of_Gross_Fixed_Capital_Formation_Value': 'Year Range_Percentage_of_Gross_Fixed_Capital_Formation'\n",
    "})\n",
    "\n",
    "# ---- Step 5: Min-Max normalization: global, regional, income group ----\n",
    "\n",
    "def minmax_norm(series):\n",
    "    minv, maxv = series.min(), series.max()\n",
    "    if pd.isnull(minv) or pd.isnull(maxv) or minv == maxv:\n",
    "        return pd.Series([None]*len(series), index=series.index)\n",
    "    return (series - minv) / (maxv - minv)\n",
    "\n",
    "num_cols = [\n",
    "    'd_Percentage_of_Gross_Domestic_Product',\n",
    "    'd_Percentage_of_Gross_Fixed_Capital_Formation'\n",
    "]\n",
    "\n",
    "# Global normalization\n",
    "for col in num_cols:\n",
    "    merged2[f\"{col}_w\"] = minmax_norm(merged2[col])\n",
    "\n",
    "# Regional normalization\n",
    "for col in num_cols:\n",
    "    merged2[f\"{col}_r\"] = (\n",
    "        merged2.groupby(\"region\")[col]\n",
    "        .transform(minmax_norm)\n",
    "    )\n",
    "\n",
    "# Income group normalization\n",
    "for col in num_cols:\n",
    "    merged2[f\"{col}_i\"] = (\n",
    "        merged2.groupby(\"incomegroup\")[col]\n",
    "        .transform(minmax_norm)\n",
    "    )\n",
    "\n",
    "# ---- Step 6: Select and reorder columns, drop unneeded ----\n",
    "\n",
    "final_cols = [\n",
    "    \"code\",\n",
    "    \"d_Percentage_of_Gross_Domestic_Product\",\n",
    "    \"Year Range_Percentage_of_Gross_Domestic_Product\",\n",
    "    \"Year Range_Percentage_of_Gross_Fixed_Capital_Formation\",\n",
    "    \"d_Percentage_of_Gross_Fixed_Capital_Formation\",\n",
    "    \"d_Percentage_of_Gross_Domestic_Product_w\",\n",
    "    \"d_Percentage_of_Gross_Fixed_Capital_Formation_w\",\n",
    "    \"d_Percentage_of_Gross_Domestic_Product_r\",\n",
    "    \"d_Percentage_of_Gross_Fixed_Capital_Formation_r\",\n",
    "    \"d_Percentage_of_Gross_Domestic_Product_i\",\n",
    "    \"d_Percentage_of_Gross_Fixed_Capital_Formation_i\"\n",
    "]\n",
    "\n",
    "out_df = merged2[final_cols].sort_values('code')\n",
    "\n",
    "# ---- Step 7: Save to CSV ----\n",
    "\n",
    "out_df.to_csv(\"FDI_d.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FTRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ftri(country_code_df):\n",
    "\n",
    "    # Get all years from 2000 to this year\n",
    "    import datetime\n",
    "    this_year = datetime.datetime.now().year\n",
    "    years = list(range(2000, this_year+1))\n",
    "    year_text = \",\".join(str(y) for y in years)\n",
    "\n",
    "    # Filter clause\n",
    "    filter_clause = (\n",
    "        \" Category/Code in ('FTRI00','FTRI01','FTRI02','FTRI03','FTRI04','FTRI05') \"\n",
    "        f\"and Year in ({year_text})\"\n",
    "    )\n",
    "\n",
    "    form_body = (\n",
    "        \"$select=Economy/Label ,Category/Label ,Year, Index_Value, Index_Footnote, Index_MissingValue\"\n",
    "        + \"&$filter=\" + requests.utils.quote(filter_clause)\n",
    "        + \"&$orderby=Economy/Order asc ,Category/Order asc ,Year asc\"\n",
    "        + \"&$compute=\" + requests.utils.quote(\n",
    "            \"round(M6700/Value div 1, 1) as Index_Value, M6700/Footnote/Text as Index_Footnote, M6700/MissingValue/Label as Index_MissingValue\"\n",
    "        )\n",
    "        + \"&$format=csv\"\n",
    "        + \"&compress=gz\"\n",
    "    )\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        \"ClientId\": client_id,\n",
    "        \"ClientSecret\": client_secret\n",
    "    }\n",
    "\n",
    "    resp = requests.post(\n",
    "        \"https://unctadstat-user-api.unctad.org/US.FTRI/cur/Facts?culture=en\",\n",
    "        data=form_body,\n",
    "        headers=headers\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    with gzip.GzipFile(fileobj=BytesIO(resp.content)) as gz:\n",
    "        df = pd.read_csv(gz)\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    df = df.drop(columns=[\"Index_Footnote\", \"Index_MissingValue\"], errors='ignore')\n",
    "\n",
    "    # Ensure correct types\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "    df[\"Index_Value\"] = pd.to_numeric(df[\"Index_Value\"], errors=\"coerce\")\n",
    "\n",
    "    # Merge with country_code_df on Economy_Label / country_unctad\n",
    "    merged = pd.merge(\n",
    "        df, country_code_df,\n",
    "        left_on=\"Economy_Label\", right_on=\"country_unctad\", how=\"right\"\n",
    "    )\n",
    "\n",
    "    # Keep only valid index values\n",
    "    filtered = merged[merged[\"Index_Value\"].notnull()]\n",
    "\n",
    "    # Group: for each country, pick the most recent year for any FTRI category\n",
    "    grouped = (\n",
    "        filtered.sort_values(\"Year\", ascending=False)\n",
    "        .groupby(\"Economy_Label\", as_index=False)\n",
    "        .first()\n",
    "    )\n",
    "\n",
    "    grouped = grouped.rename(columns={\"Index_Value\": \"FTRI\"})\n",
    "\n",
    "    # Min-max normalization: global\n",
    "    def minmax_norm(series):\n",
    "        minv, maxv = series.min(), series.max()\n",
    "        if pd.isnull(minv) or pd.isnull(maxv) or minv == maxv:\n",
    "            return pd.Series([None]*len(series), index=series.index)\n",
    "        return (series - minv) / (maxv - minv)\n",
    "\n",
    "    grouped[\"FTRI_w\"] = minmax_norm(grouped[\"FTRI\"])\n",
    "    # Regional normalization\n",
    "    grouped[\"FTRI_r\"] = grouped.groupby(\"region\")[\"FTRI\"].transform(minmax_norm)\n",
    "    # Income group normalization\n",
    "    grouped[\"FTRI_i\"] = grouped.groupby(\"incomegroup\")[\"FTRI\"].transform(minmax_norm)\n",
    "\n",
    "    # Select and reorder columns\n",
    "    final_cols = [\n",
    "        \"code\", \"Year\", \"FTRI\", \"FTRI_w\", \"FTRI_r\", \"FTRI_i\"\n",
    "    ]\n",
    "    out_df = grouped[final_cols].sort_values('code')\n",
    "\n",
    "    out_df.to_csv(\"FTRI.csv\", index=False)\n",
    "    return out_df\n",
    "\n",
    "# ---- Usage ----\n",
    "result = load_ftri(country_code_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FTRI_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ftri_d(country_code_df):\n",
    "\n",
    "    # All years from 2000 to present\n",
    "    import datetime\n",
    "    this_year = datetime.datetime.now().year\n",
    "    years = list(range(2000, this_year + 1))\n",
    "    year_text = \",\".join(str(y) for y in years)\n",
    "\n",
    "    # Filter clause\n",
    "    filter_clause = (\n",
    "        \" Category/Code in ('FTRI00','FTRI01','FTRI02','FTRI03','FTRI04','FTRI05') \"\n",
    "        f\"and Year in ({year_text})\"\n",
    "    )\n",
    "\n",
    "    form_body = (\n",
    "        \"$select=Economy/Label ,Category/Label ,Year, Index_Value, Index_Footnote, Index_MissingValue\"\n",
    "        + \"&$filter=\" + requests.utils.quote(filter_clause)\n",
    "        + \"&$orderby=Economy/Order asc ,Category/Order asc ,Year asc\"\n",
    "        + \"&$compute=\" + requests.utils.quote(\n",
    "            \"round(M6700/Value div 1, 1) as Index_Value, M6700/Footnote/Text as Index_Footnote, M6700/MissingValue/Label as Index_MissingValue\"\n",
    "        )\n",
    "        + \"&$format=csv\"\n",
    "        + \"&compress=gz\"\n",
    "    )\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        \"ClientId\": client_id,\n",
    "        \"ClientSecret\": client_secret\n",
    "    }\n",
    "\n",
    "    resp = requests.post(\n",
    "        \"https://unctadstat-user-api.unctad.org/US.FTRI/cur/Facts?culture=en\",\n",
    "        data=form_body,\n",
    "        headers=headers\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    with gzip.GzipFile(fileobj=BytesIO(resp.content)) as gz:\n",
    "        df = pd.read_csv(gz)\n",
    "\n",
    "    # Remove unnecessary columns and ensure types\n",
    "    df = df.drop(columns=[\"Index_Footnote\", \"Index_MissingValue\"], errors='ignore')\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\")\n",
    "    df[\"Index_Value\"] = pd.to_numeric(df[\"Index_Value\"], errors=\"coerce\")\n",
    "\n",
    "    # Average index value per country and year (all categories)\n",
    "    grouped = (\n",
    "        df.groupby([\"Economy_Label\", \"Year\"], as_index=False)\n",
    "        .agg(Average_Index_Value=(\"Index_Value\", \"mean\"))\n",
    "    )\n",
    "\n",
    "    # Remove nulls\n",
    "    grouped = grouped[grouped[\"Average_Index_Value\"].notnull()]\n",
    "\n",
    "    # Only last 5 years per country\n",
    "    latest_year = grouped[\"Year\"].max()\n",
    "    cut_year = latest_year - 4\n",
    "    window5 = grouped[grouped[\"Year\"] >= cut_year]\n",
    "\n",
    "    # Sort by country and year\n",
    "    window5 = window5.sort_values([\"Economy_Label\", \"Year\"])\n",
    "\n",
    "    # Summarize: for each country, calculate start/end values and growth rate\n",
    "    summarized = (\n",
    "        window5.groupby(\"Economy_Label\")\n",
    "        .agg(\n",
    "            FirstYear=(\"Year\", \"min\"),\n",
    "            LastYear=(\"Year\", \"max\"),\n",
    "            StartVal=(\"Average_Index_Value\", \"first\"),\n",
    "            EndVal=(\"Average_Index_Value\", \"last\")\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    summarized[\"d_FTRI\"] = (\n",
    "        (summarized[\"EndVal\"] - summarized[\"StartVal\"]) /\n",
    "        (summarized[\"LastYear\"] - summarized[\"FirstYear\"])\n",
    "    )\n",
    "    summarized[\"Year Range_FTRI\"] = (\n",
    "        summarized[\"FirstYear\"].astype(str) + \"–\" + summarized[\"LastYear\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # Keep only needed columns\n",
    "    final = summarized[[\"Economy_Label\", \"d_FTRI\", \"Year Range_FTRI\"]]\n",
    "\n",
    "    # Merge with country codes\n",
    "    merged = pd.merge(\n",
    "        final, country_code_df,\n",
    "        left_on=\"Economy_Label\", right_on=\"country_unctad\", how=\"right\"\n",
    "    )\n",
    "\n",
    "    # ---- Min-Max Normalization ----\n",
    "\n",
    "    def minmax_norm(series):\n",
    "        minv, maxv = series.min(), series.max()\n",
    "        if pd.isnull(minv) or pd.isnull(maxv) or minv == maxv:\n",
    "            return pd.Series([None]*len(series), index=series.index)\n",
    "        return (series - minv) / (maxv - minv)\n",
    "\n",
    "    num_cols = [\"d_FTRI\"]\n",
    "\n",
    "    for col in num_cols:\n",
    "        merged[f\"{col}_w\"] = minmax_norm(merged[col])\n",
    "        merged[f\"{col}_r\"] = merged.groupby(\"region\")[col].transform(minmax_norm)\n",
    "        merged[f\"{col}_i\"] = merged.groupby(\"incomegroup\")[col].transform(minmax_norm)\n",
    "\n",
    "    # Select and reorder columns\n",
    "    final_cols = [\n",
    "        \"code\", \"d_FTRI\", \"Year Range_FTRI\", \"d_FTRI_w\", \"d_FTRI_r\", \"d_FTRI_i\"\n",
    "    ]\n",
    "    out_df = merged[final_cols].sort_values(\"code\")\n",
    "\n",
    "    out_df.to_csv(\"ftri_d.csv\", index=False)\n",
    "    return out_df\n",
    "\n",
    "# ---- Usage ----\n",
    "result = load_ftri_d(country_code_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to Excel\n",
    "Get all the .csv in this folder and generate an excel with every sheet is a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully combined 11 CSV files into combined_results.xlsx\n",
      "CSV files processed:\n",
      "- bop.csv\n",
      "- bop_d.csv\n",
      "- country_codes.csv\n",
      "- CPIA.csv\n",
      "- gfdd.csv\n",
      "- gfdd_d.csv\n",
      "- ic_frm.csv\n",
      "- LPI.csv\n",
      "- LPI_d.csv\n",
      "- world_bank_data.csv\n",
      "- world_bank_data_final.csv\n"
     ]
    }
   ],
   "source": [
    "# Get all the .csv in this folder and generate an excel with every sheet is a csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get all CSV files in current directory\n",
    "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "\n",
    "# Create Excel writer object\n",
    "with pd.ExcelWriter('combined_results.xlsx', engine='openpyxl') as writer:\n",
    "    # Read each CSV and write to a sheet\n",
    "    for csv_file in csv_files:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Get sheet name (remove .csv extension)\n",
    "        sheet_name = os.path.splitext(csv_file)[0]\n",
    "        \n",
    "        # Write to Excel sheet\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Successfully combined {len(csv_files)} CSV files into combined_results.xlsx\")\n",
    "print(\"CSV files processed:\")\n",
    "for csv in csv_files:\n",
    "    print(f\"- {csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
